---
layout: post
title: "Java 集合类"
categories: [Java]
description:
keywords:
---

* content
{:toc}


## HashMap

众所周知, HashMap是一个用于存储Key-Value键值对的集合, 每一个键值对也叫做**Entry**.这些个键值对(Entry)分散存储在一个数组当中, 这个数组就是HashMap的主干.

HashMap数组每一个元素的初始值都是Null.

![http://www.milky.show/images/java/collection/hashmap/hashmap_1.png](http://www.milky.show/images/java/collection/hashmap/hashmap_1.png)

对于 HashMap, 我们最常使用的是两个方法: **Get** 和 **Put**.**利用 hashCode 方法找到链表, 利用 equals 方法找到重复的 key**

**Put 方法的原理**

调用 Put 方法的时候发生了什么呢?

比如调用 hashMap.put("apple", 0), 插入一个Key为“apple"的元素.这时候我们需要利用一个哈希函数来确定 Entry 的插入位置(index): 

**index = Hash(“apple”)**

假定最后计算出的 index 是 2, 那么结果如下: 

![http://www.milky.show/images/java/collection/hashmap/hashmap_2.png](http://www.milky.show/images/java/collection/hashmap/hashmap_2.png)

**但是, 因为 HashMap 的长度是有限的, 当插入的 Entry 越来越多时, 再完美的 Hash 函数也难免会出现 index 冲突的情况.比如下面这样: **

![http://www.milky.show/images/java/collection/hashmap/hashmap_3.png](http://www.milky.show/images/java/collection/hashmap/hashmap_3.png)

这时候该怎么办呢? 我们可以利用**链表**来解决.

HashMap 数组的每一个元素不止是一个 Entry 对象, 也是一个链表的头节点.每一个 Entry 对象通过 Next 指针指向它的下一个 Entry 节点.当新来的 Entry 映射到冲突的数组位置时, 只需要插入到对应的链表即可: 

![http://www.milky.show/images/java/collection/hashmap/hashmap_4.png](http://www.milky.show/images/java/collection/hashmap/hashmap_4.png)

**在链表中插入的时候会调用 equals 方法比较两个 key 是否为同一个元素, 如果相等会覆盖原有的数据**

**需要注意的是, 新来的Entry节点插入链表时, 使用的是“头插法”.至于为什么不插入链表尾部, 后面会有解释.**



**Get 方法的原理**

使用 Get 方法根据 Key 来查找 Value 的时候, 发生了什么呢?

首先会把输入的 Key 做一次 Hash 映射, 得到对应的 index: 

**index = Hash(“apple”)**

由于刚才所说的 Hash 冲突, 同一个位置有可能匹配到多个 Entry, 这时候就需要顺着对应链表的头节点, 一个一个向下来查找.假设我们要查找的 Key 是“apple”: 

![http://www.milky.show/images/java/collection/hashmap/hashmap_5.png](http://www.milky.show/images/java/collection/hashmap/hashmap_5.png)

第一步, 我们查看的是头节点 Entry6, Entry6 的 Key 是 banana, 显然不是我们要找的结果.

第二步, 我们查看的是 Next 节点 Entry1, Entry1 的 Key 是 apple, 正是我们要找的结果.

**之所以把 Entry6 放在头节点, 是因为 HashMap 的发明者认为, 后插入的 Entry 被查找的可能性更大**.



**HashMap 的初始长度**

**HashMap 的默认初始长度是 16, 并且每次自动扩展或是手动初始化时, 长度必须是 2 的幂**

**之所以选择 16, 是为了服务于从 Key 映射到 index 的 hash 算法**

**index = Hash(“apple”)**

如何实现一个尽量均匀分布的Hash函数呢?我们通过利用 Key 的 HashCode 值来做某种运算.

**index = HashCode(Key) % Length**, 这种取模运算很简单, 但是效率很低. 为了实现高效的 Hash 算法, HashMap 发明者采用了位运算的方式

如何进行位运算呢?有如下的公式(Length 是 HashMap的长度): 

**index = HashCode(Key) & (Length - 1)** 

下面我们以值为 “book” 的 Key 来演示整个过程: 

1. 计算 book 的 hashcode, 结果为十进制的 3029737, 二进制的 101110001110101110 1001
2. 假定HashMap长度是默认的 16, 计算 Length - 1 的结果为十进制的 15, 二进制的 1111

3. 把以上两个结果做**与运算**, 101110001110101110 1001 & 1111 = 1001, 十进制是 9, 所以 index = 9

可以说, Hash 算法最终得到的 index 结果, 完全取决于 Key 的 Hashcode 值的最后几位

**这样做的效果上等同于取模, 而且还大大提高了性能, 至于为什么采用 16, 我们可以试试长度是 10 会出现什么问题**

假设HashMap的长度是 10, 重复刚才的运算步骤: 

1. 计算 book 的 hashcode, 结果为十进制的 3029737, 二进制的 101110001110101110 1001
2. HashMap 的长度是 10, 计算 Length - 1 的结果为十进制的 9, 二进制 1001
3. 把以上两个结果做**与运算**, 101110001110101110 1001 & 1001 = 1001, 十进制是 9, 所以 index = 9

单独看这个结果, 表面上并没有问题.我们再来尝试一个新的HashCode 101110001110101110 **1011:** 结果为 1001

让我们再换一个HashCode 101110001110101110 **1111:**  结果为 1001

**虽然 HashCode 的倒数第二第三位从  0 变成了 1, 但是运算的结果都是 1001.也就是说, 当 HashMap 长度为 10 的时候, 有些index 结果的出现几率会更大, 而有些 index 结果永远不会出现(比如 0111), 因为二三位是 0, 永远 & 不出 1 来**

**这样, 显然不符合Hash算法均匀分布的原则**

**反观长度 16 或者其他 2 的幂, Length - 1 的值是所有二进制位全为 1, 这种情况下, index 的结果等同于 HashCode 后几位的值.只要输入的 HashCode 本身分布均匀, Hash 算法的结果就是均匀的**

**高并发下的 HashMap**

HashMap 的容量是有限的.当经过多次元素插入, 使得 HashMap 达到一定饱和度时, Key映射位置发生冲突的几率会逐渐提高.

这时候, HashMap 需要扩展它的长度, 也就是进行 **Resize**

**影响发生 Resize 的因素有两个: **

1. Capacity

    HashMap 的当前长度, HashMap 的长度是 2 的幂, 默认是 16

2. LoadFactor

    HashMap 负载因子, 默认值为 0.75f

**衡量 HashMap 是否进行 Resize 的条件如下: **

**HashMap.Size  >= Capacity \* LoadFactor**

**HashMap 的 resize 操作要经过下面两个步骤**

1. 扩容

    创建一个新的 Entry 空数组, 长度是原来数组的 2 倍

2. ReHash

    遍历原Entry数组, 把所有的 Entry 重新 Hash 到新数组.为什么要重新 Hash 呢?因为长度扩大以后, Hash 的规则也随之改变.

让我们回顾一下 Hash 公式: 

**index = HashCode(key) & (Length - 1)**

当原数组长度为 8 时, Hash 运算是和 111B 做与运算; 新数组长度为 16, Hash 运算是和 1111B 做与运算.Hash 结果显然不同.

**Resize 前的 HashMap:**

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_6.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_6.png" style="zoom:67%;" />

**Resize 后的 HashMap:**

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_7.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_7.png" style="zoom:67%;" />



**ReHash 的 Java 代码如下**

```java
void transfer(Entry[] newTable, boolean rehash) {
    int newCapacity = newTable.length;
    for (Entry<K,V> e : table) {
        while(null != e) {
            Entry<K,V> next = e.next;
            if (rehash) {
                e.hash = null == e.key ? 0 : hash(e.key);
            }
            int i = indexFor(e.hash, newCapacity);
            e.next = newTable[i];
            newTable[i] = e;
            e = next;
        }
    }
}
```

上述流程在单线程下没有什么问题, 但是 HashMap 并不是线程安全的, 在多线程环境下, rehash 操作会带来线程安全问题

假设一个 HashMap 已经到了 Resize 的临界点.此时有两个线程 A 和 B, 在同一时刻对 HashMap 进行 Put 操作: 

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_8.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_8.png" style="zoom:67%;" />

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_9.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_9.png" style="zoom:67%;" />

此时达到 Resize 条件, 两个线程各自进行 Rezie 的第一步, 也就是扩容: 

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_10.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_10.png" style="zoom:67%;" />

这时候, 两个线程都走到了 ReHash 的步骤.让我们回顾一下 ReHash 的代码: 

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_11.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_11.png" style="zoom:67%;" />

假如此时线程 B 遍历到 Entry3 对象, 刚执行完红框里的这行代码, 线程就被挂起.对于线程 B 来说: 

**e = Entry3**

**next = Entry2**

这时候线程 A 畅通无阻地进行着 ReHash, 当 ReHash 完成后, 结果如下(图中的 e 和 next, 代表线程 B 的两个引用)

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_12.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_12.png" style="zoom: 67%;" />

直到这一步, 看起来没什么毛病.接下来线程 B 恢复, 继续执行属于它自己的 ReHash.线程 B 刚才的状态是: 

**e = Entry3**

**next = Entry2**

![http://www.milky.show/images/java/collection/hashmap/hashmap_13.png](http://www.milky.show/images/java/collection/hashmap/hashmap_13.png)

当执行到上面这一行时, 显然 i = 3, 因为刚才线程 A 对于 Entry3 的 hash 结果也是 3.

![http://www.milky.show/images/java/collection/hashmap/hashmap_14.png](http://www.milky.show/images/java/collection/hashmap/hashmap_14.png)

我们继续执行到这两行, Entry3 放入了线程 B 的数组下标为 3 的位置, 并且 **e 指向了 Entry2**.此时 e 和 next 的指向如下: 

**e = Entry2**

**next = Entry2**

整体情况如图所示: 

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_15.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_15.png" style="zoom:67%;" />

接着是新一轮循环, 又执行到红框内的代码行: 

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_16.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_16.png" style="zoom:67%;" />

**e = Entry2**

**next = Entry3**

整体情况如图所示: 

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_17.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_17.png" style="zoom:67%;" />

接下来执行下面的三行, 用头插法把 Entry2 插入到了线程B的数组的头结点: 

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_18.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_18.png" style="zoom:67%;" />

整体情况如图所示: 

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_19.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_19.png" style="zoom:67%;" />

第三次循环开始, 又执行到红框的代码: 

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_20.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_20.png" style="zoom:67%;" />

**e = Entry3**

**next = Entry3.next = null**

最后一步, 当我们执行下面这一行的时候, 见证奇迹的时刻来临了: 

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_21.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_21.png" style="zoom:67%;" />

**newTable[i] = Entry2**

**e = Entry3**

**Entry2.next = Entry3**

**Entry3.next = Entry2**

**链表出现了环形！**

整体情况如图所示: 

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_22.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_22.png" style="zoom:67%;" />

**此时, 问题还没有直接产生.当调用 Get 查找一个不存在的 Key, 而这个 Key 的 Hash 结果恰好等于 3 的时候, 由于位置 3 带有环形链表, 所以程序将会进入死循环！**



**容量与哈希**

在Java中, 保存数据有两种比较简单的数据结构: 数组和链表.数组的特点是: 寻址容易, 插入和删除困难; 而链表的特点是: 寻址困难, 插入和删除容易.HashMap就是将数组和链表组合在一起, 发挥了两者的优势, 我们可以将其理解为链表的数组.

在HashMap中, 有两个比较容易混淆的关键字段: size和capacity , 这其中capacity就是Map的容量, 而size我们称之为Map中的元素个数.

简单打个比方你就更容易理解了: HashMap就是一个“桶”, 那么容量(capacity)就是这个桶当前最多可以装多少元素, 而元素个数(size)表示这个桶已经装了多少元素.

当我们创建一个HashMap的时候, 如果没有指定其容量, 那么会得到一个默认容量为16的Map, 那么, 这个容量是怎么来的呢?又为什么是这个数字呢?

我们知道, 容量就是一个HashMap中"桶"的个数, 那么, 当我们想要往一个HashMap中put一个元素的时候, 需要通过一定的算法计算出应该把他放到哪个桶中, 这个过程就叫做哈希(hash), 对应的就是HashMap中的hash方法.

我们知道, hash方法的功能是根据Key来定位这个K-V在链表数组中的位置的.也就是hash方法的输入应该是个Object类型的Key, 输出应该是个int类型的数组下标.如果让你设计这个方法, 你会怎么做?

其实简单, 我们只要调用Object对象的hashCode()方法, 该方法会返回一个整数, 然后用这个数对HashMap的容量进行取模就行了.

如果真的是这么简单的话, 那HashMap的容量设置就会简单很多了, 但是考虑到效率等问题, HashMap的hash方法实现还是有一定的复杂的.



**指定容量初始化**

当我们通过 HashMap(int initialCapacity) 设置初始容量的时候, HashMap 并不一定会直接采用我们传入的数值, 而是经过计算, 得到一个新值, 目的是提高 hash 的效率.(1->1、3->4、7->8、9->16)

```java
int n = cap - 1;
n |= n >>> 1;
n |= n >>> 2;
n |= n >>> 4;
n |= n >>> 8;
n |= n >>> 16;
return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
```

上面的算法目的挺简单, 就是: 根据用户传入的容量值(代码中的cap), 通过计算, 得到第一个比他大的 2 的幂并返回.

**总之, HashMap 根据用户传入的初始化容量, 利用无符号右移和按位或运算等方式计算出第一个大于该数的2的幂.**

**扩容**

HashMap 有扩容机制, 就是当达到扩容条件时会进行扩容. HashMap 的扩容条件就是当 HashMap 中的元素个数(size)超过临界值(threshold)时就会自动扩容.

**在 HashMap 中, threshold = loadFactor * capacity.**

**loadFactor 是装载因子, 表示 HashMap 满的程度, 默认值为 0.75f, 设置成 0.75 有一个好处, 那就是 0.75 正好是 3/4, 而capacity 又是 2 的幂.所以, 两个数的乘积都是整数.**

对于一个默认的 HashMap 来说, 默认情况下, 当其 size 大于12(16*0.75)时就会触发扩容, 每次容量扩为原来的 2 倍, 即从 16 扩容到 32, 64, 128...

所以, 通过保证初始化容量均为 2 的幂, 并且扩容时也是扩容到之前容量的 2 倍, 所以, 保证了 HashMap 的容量永远都是2的幂, **这样进行 hash 运算时可以保证很高的效率.**

**在合适的时候扩大数组容量, 再通过一个合适的 hash 算法计算元素分配到哪个数组中, 就可以大大的减少哈希冲突的概率.就能避免查询效率低下的问题, 扩容就是为了减少哈希冲突, 提高性能.**

**负载因子(loadFactor)为什么是 0.75?**

从代码中我们可以看到, 在向 HashMap 中添加元素过程中, 如果 `元素个数(size)超过临界值(threshold)` 的时候, 就会进行自动扩容(resize), 并且, 在扩容之后, 还需要对HashMap中原有元素进行rehash, 即将原来桶中的元素重新分配到新的桶中.

在 HashMap 中, 临界值(threshold) = 负载因子(loadFactor) * 容量(capacity).

**那么, 为什么选择 0.75 呢?背后有什么考虑?为什么不是 1, 不是 0.8?不是 0.5, 而是 0.75 呢?**

**Java 官方的解释说, 默认的负载因子(0.75)在时间和空间成本之间提供了很好的权衡.更高的值减少了空间开销, 但增加了查找成本(反映在 HashMap 类的大多数操作中, 包括 get 和 put).**

试想一下, 如果我们把负载因子设置成 1, 容量使用默认初始值 16, 那么表示一个 HashMap 需要在"满了"之后才会进行扩容.

那么在 HashMap 中, 最好的情况是这 16 个元素通过 hash 算法之后分别落到了 16 个不同的桶中, 否则就必然发生哈希碰撞.而且随着元素越多, 哈希碰撞的概率越大, 查找速度也会越低.

**负载因子表示一个数组可以达到的最大的满的程度.这个值不宜太大, 也不宜太小.**

**loadFactor太大, 比如等于 1, 那么就会有很高的哈希冲突的概率, 会大大降低查询速度.**

**loadFactor太小, 比如等于 0.5, 那么频繁扩容没, 就会大大浪费空间.**

**所以, 这个值需要介于 0.5 和 1 之间.根据数学公式推算.这个值在 log(2) 的时候比较合理.**

**另外, 为了提升扩容效率, HashMap 的容量(capacity)有一个固定的要求, 那就是一定是 2 的幂.**

**所以, 如果 loadFactor 是 3/4 的话, 那么和 capacity 的乘积结果就可以是一个整数.**



**哈希表如何解决 Hash 冲突**

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_23.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_23.png" style="zoom:67%;" />

**为什么 HashMap 具备下述特点:键-值(key-value)都允许为空、线程不安全、不保证有序、存储位置随时间变化**

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_24.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_24.png" style="zoom:67%;" />

**为什么 HashMap 中 String、Integer 这样的包装类适合作为 key 键**

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_25.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_25.png" style="zoom:67%;" />

**HashMap 中的 key 若 Object 类型, 则需实现哪些方法?**

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_26.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_26.png" style="zoom:67%;" />



**HashMap 之 1.7 和 1.8 的区别**

1.  底层数据结构不一样, 1.7是**数组 + 链表**, 1.8 则是**数组 + 链表 + 红黑树结构(当链表长度大于 8, 转为红黑树). **
2.  JDK1.8 中 resize() 方法在表为空时, 创建表; 在表不为空时, 扩容; 而 JDK1.7 中 resize() 方法负责扩容, inflateTable() 负责创建表. 
3.   1.8 中没有区分键为 null 的情况, 而 1.7 版本中对于键为 null 的情况调用 putForNullKey() 方法. 但是两个版本中如果键为 null, 那么调用 hash() 方法得到的都将是 0, **所以键为 null 的元素都始终位于哈希表 table[0] 中. **
4.  当 1.8 中的桶中元素处于链表的情况, 遍历的同时最后如果没有匹配的, 直接将节点添加到链表尾部; 而 1.7 在遍历的同时没有添加数据, 而是另外调用了 addEntry() 方法, 将节点添加到链表头部. 
5.  1.7 中新增节点采用头插法, 1.8 中新增节点采用尾插法. 这也是为什么 1.8 不容易出现环型链表的原因. 
6.  1.7 中是通过更改 hashSeed 值修改节点的hash值从而达到rehash时的链表分散, 而 1.8 中键的 hash 值不会改变, rehash 时根据(hash&oldCap)==0 将链表分散. 
7.   1.8 rehash 时保证原链表的顺序, 而 1.7 中 rehash 时有可能改变链表的顺序(头插法导致). 
8.  **在扩容的时候: 1.7 在插入数据之前扩容, 而 1.8 插入数据成功之后扩容. **



**HashMap 的工作原理**

HashMap 底层是 hash 数组和单向链表实现, 数组中的每个元素都是链表, 由 Node 内部类(实现 Map.Entry接口)实现, HashMap 通过 put & get 方法存储和获取. 

存储对象时, 将 K/V 键值传给 put() 方法: 

1.  调用 hash(K) 方法计算 K 的 hash 值, 然后结合数组长度, 计算得数组下标; 

2.  调整数组大小(当容器中的元素个数大于 capacity * loadfactor 时, 容器会进行扩容resize 为 2n); 

3.  如果 K 的 hash 值在 HashMap 中不存在, 则执行插入, 若存在, 则发生碰撞; 

    如果 K 的 hash 值在 HashMap 中存在, 且它们两者 equals 返回 true, 则更新键值对; 

    如果 K 的 hash 值在 HashMap 中存在, 且它们两者 equals 返回 false, 则插入链表的尾部(尾插法)或者红黑树中(树的添加方式). 

(JDK 1.7 之前使用头插法、JDK 1.8 使用尾插法)(注意: 当碰撞导致链表大于 TREEIFY_THRESHOLD = 8 时, 就把链表转换成红黑树)

获取对象时, 将 K 传给 get() 方法: ①、调用 hash(K) 方法(计算 K 的 hash 值)从而获取该键值所在链表的数组下标; ②、顺序遍历链表, equals()方法查找相同 Node 链表中 K 值对应的 V 值. 

hashCode 是定位的, 存储位置; equals是定性的, 比较两者是否相等. 



**HashMap 的 hash 算法的实现原理**

JDK 1.8 中, 是通过 hashCode() 的高 16 位异或低 16 位实现的: (h = k.hashCode()) ^ (h >>> 16), 主要是从速度, 功效和质量来考虑的, 减少系统的开销, 也不会造成因为高位没有参与下标的计算, 从而引起的碰撞. 



这个也是数学的范畴, 从我们的角度来讲, 只要知道这是为了更好的均匀散列表的下标就好了, 我们来看看 HashMap 的 hash 算法(JDK 8).

```java
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

乍看一下就是简单的异或运算和右移运算, 但是为什么要异或呢? 为什么要移位呢? 而且移位16? 

在分析这个问题之前, 我们需要先看看另一个事情,  **HashMap 如何根据 hash 值找到数组中的对象**, 我们看看 get 方法的代码: 

```java
final Node<K,V> getNode(int hash, Object key) {
    Node<K,V>[] tab; Node<K,V> first, e; int n; K k;
    if ((tab = table) != null && (n = tab.length) > 0 &&
            (first = tab[(n - 1) & hash]) != null) {
        if (first.hash == hash && // always check first node
                ((k = first.key) == key || (key != null && key.equals(k))))
            return first;
        if ((e = first.next) != null) {
            if (first instanceof TreeNode)
                return ((HashMap.TreeNode<K,V>)first).getTreeNode(hash, key);
            do {
                if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                    return e;
            } while ((e = e.next) != null);
        }
    }
    return null;
}
```

我们看看代码中注释下方的一行代码: first = tab[(n - 1) & hash]). 

使用数组长度减一 与运算 hash 值. 这行代码就是为什么要让前面的 hash 方法移位并异或. 

我们分析一下: 

首先, 假设有一种情况, 对象 A 的 hashCode 为 1000010001110001000001111000000, 对象 B 的 hashCode 为 0111011100111000101000010100000. 

如果数组长度是16, 也就是 15 与运算这两个数,  你会发现结果都是0. 这样的散列结果太让人失望了. 很明显不是一个好的散列算法. 

但是如果我们将 hashCode 值右移 16 位, 也就是取 int 类型的一半, 刚好将该二进制数对半切开. 并且使用位异或运算(如果两个数对应的位置相反, 则结果为1, 反之为0), 这样的话, 就能避免我们上面的情况的发生. 

总的来说, 使用位移 16 位和 异或 就是防止这种极端情况. 但是, 该方法在一些极端情况下还是有问题, 比如: 10000000000000000000000000 和 10000000001000000000000000 这两个数, 如果数组长度是 16, 那么即使右移 16 位, 在异或, hash 值还是会重复. 但是为了性能, 对这种极端情况, JDK 的作者选择了性能. 毕竟这是少数情况, 为了这种情况去增加 hash 时间, 性价比不高. 



**HashMap 为什么使用 & 与运算代替模运算**

我们再看看刚刚说的那个根据hash计算下标的方法: 

tab[(n - 1) & hash]; 

其中 n 是数组的长度. 其实该算法的结果和模运算的结果是相同的. 但是, 对于现代的处理器来说, 除法和求余数(模运算)是最慢的动作. 

上面情况下和模运算相同

a % b == (b-1) & a ,当b是2的指数时, 等式成立. 

我们说 & 与运算的定义: 与运算 第一个操作数的的第n位于第二个操作数的第n位如果都是1, 那么结果的第n为也为1, 否则为0; 

当 n 为 16 时,  与运算 101010100101001001101 时, 也就是
1111 & 101010100101001001000 结果: 1000 = 8
1111 & 101000101101001001001 结果: 1001 = 9
1111 & 101010101101101001010 结果:  1010 = 10
1111 & 101100100111001101100 结果:  1100 = 12

可以看到, 当 n 为 2 的幂次方的时候, 减一之后就会得到 1111* 的数字, 这个数字正好可以掩码. 并且得到的结果取决于 hash 值. 因为 hash 值是1, 那么最终的结果也是1 , hash 值是0, 最终的结果也是0. 



**HashMap 的容量为什么建议是 2 的幂次方**

我们说, hash 算法的目的是为了让hash值均匀的分布在桶中(数组), 那么, 如何做到呢? 试想一下, 如果不使用 2 的幂次方作为数组的长度会怎么样? 

假设我们的数组长度是 10, 还是上面的公式: 
1010 & 101010100101001001000 结果: 1000 = 8
1010 & 101000101101001001001 结果: 1000 = 8
1010 & 101010101101101001010 结果:  1010 = 10
1010 & 101100100111001101100 结果:  1000 = 8

看到结果我们惊呆了, 这种散列结果, 会导致这些不同的key值全部进入到相同的插槽中, 形成链表, 性能急剧下降. 

所以说, 我们一定**要保证 & 中的二进制位全为 1**, 才能最大限度的利用 hash 值, 并更好的散列, 只有全是1, 才能有更多的散列结果. 如果是 1010, 有的散列结果是永远都不会出现的, 比如 0111, 0101, 1111, 1110…, 只要 & 之前的数有 0, 对应的 1 肯定就不会出现(因为只有都是1才会为1). 大大限制了散列的范围. 



**我们自定义 HashMap 容量最好是多少**

绝对不行, 如果大家看过源码就会发现, **如果 Map 中已有数据的容量达到了初始容量的 75%, 那么散列表就会扩容**, 而扩容将会重新将所有的数据**重新散列, 性能损失严重**, 所以, 我们可以必须要大于我们预计数据量的 1**.34 倍**, 如果是2个数据的话, 就需要初始化 2.68 个容量. 当然这是开玩笑的, 2.68 不可以, 3 可不可以呢? 肯定也是不可以的, 我前面说了, 如果不是2的幂次方, 散列结果将会大大下降. 导致出现大量链表. 那么我可以将初始化容量设置为4.  当然了, 如果你预计大概会插入 12 条数据的话, 那么初始容量为16简直是完美, 一点不浪费, 而且也不会扩容. 

如果某 个map 很大, 注意, 肯定是事先没有定义好初始化长度, 假设, 某个 Map 存储了 10000 个数据, 那么他会扩容到 20000, 实际上, 根本不用 20000, 只需要 10000 * 1.34 = 13400 个, 然后向上找到一个 2 的幂次方, 也就是 16384 初始容量足够. 

在日常开发中, 可以使用

```java
Map<String, String> map = Maps.newHashMapWithExpectedSize(10);
```

来创建一个HashMap, 计算的过程guava会帮我们完成, 这是一种用内存换性能的做法, 真正使用的时候, 要考虑到内存的影响. 

**在JDK 8中, putAll方法采用了这种方式, 而put、构造函数等并没有默认使用这个公式.**



**HashMap 的 table 的容量如何确定? loadFactor 是什么? 该容量如何变化? 这种变化会带来什么问题?** 

1.  table 数组大小是由 capacity 这个参数确定的, 默认是16, 也可以构造时传入, 最大限制是1<<30
2.  loadFactor 是装载因子, 主要目的是用来确认table 数组是否需要动态扩展, 默认值是0.75, 比如table 数组大小为 16, 装载因子为 0.75 时, threshold 就是12, 当 table 的实际大小超过 12 时, table就需要动态扩容
3.  扩容时, 调用 resize() 方法, 将 table 长度变为原来的两倍(注意是 table 长度, 而不是 threshold)

4.  如果数据很大的情况下, 扩展时将会带来性能的损失, 在性能要求很高的地方, 这种损失很可能很致命. 

**HashMap 中 put 方法的过程?** 

答: “调用哈希函数获取 Key 对应的 hash 值, 再计算其数组下标; 

如果没有出现哈希冲突, 则直接放入数组; 如果出现哈希冲突, 则以链表的方式放在链表后面; 

如果链表长度超过阀值( TREEIFY THRESHOLD==8), 就把链表转成红黑树, 链表长度低于 6, 就把红黑树转回链表;

如果结点的 key 已经存在, 则替换其 value 即可; 

如果集合中的键值对大于 12, 调用 resize 方法进行数组扩容. ”

**数组扩容的过程?** 

创建一个新的数组, 其容量为旧数组的两倍, 并重新计算旧数组中结点的存储位置. 结点在新数组中的位置只有两种, 原下标位置或原下标+旧数组的大小. 



**拉链法导致的链表过深问题为什么不用二叉查找树代替, 而选择红黑树? 为什么不一直使用红黑树?** 

之所以选择红黑树是为了解决二叉查找树的缺陷, 二叉查找树在特殊情况下会变成一条线性结构(这就跟原来使用链表结构一样了, 造成很深的问题), 遍历查找会非常慢. 推荐: 面试问红黑树, 我脸都绿了. 

而红黑树在插入新数据后可能需要通过左旋, 右旋、变色这些操作来保持平衡, 引入红黑树就是为了查找数据快, 解决链表查询深度的问题, 我们知道红黑树属于平衡二叉树, 但是为了保持“平衡”是需要付出代价的, 但是该代价所损耗的资源要比遍历线性链表要少, 所以当长度大于8的时候, 会使用红黑树, 如果链表长度很短的话, 根本不需要引入红黑树, 引入反而会慢. 



**jdk8 中对 HashMap 做了哪些改变?** 

在 java 1.8 中, 如果链表的长度超过了 8, 那么链表将转换为红黑树. (桶的数量必须大于 64, 小于 64 的时候只会扩容)

发生 hash 碰撞时, java 1.7 会在链表的头部插入, 而 java 1.8 会在链表的尾部插入

在 java 1.8 中, Entry 被 Node 替代(换了一个马甲). 



**HashMap & TreeMap & LinkedHashMap 使用场景?**

一般情况下, 使用最多的是 HashMap

HashMap: 在 Map 中插入、删除和定位元素时

TreeMap: 在需要按自然顺序或自定义顺序遍历键的情况下

LinkedHashMap: 在需要输出的顺序和输入的顺序相同的情况下



**HashMap 和 HashTable 有什么区别?** 

HashMap 是线程不安全的, HashTable 是线程安全的; 

由于线程安全, 所以 HashTable 的效率比不上 HashMap; 

HashMap 最多只允许一条记录的键为 null, 允许多条记录的值为null, 而 HashTable不允许; 

HashMap 默认初始化数组的大小为 16, HashTable 为 11, 前者扩容时, 扩大两倍, 后者扩大两倍 + 1; 

HashMap 需要重新计算 hash 值, 而 HashTable 直接使用对象的 hashCode



**Java 中的另一个线程安全的与 HashMap 极其类似的类是什么? 同样是线程安全, 它与 HashTable 在线程同步上有什么不同?** 

ConcurrentHashMap 类(是 Java并发包 java.util.concurrent 中提供的一个线程安全且高效的 HashMap 实现). 

HashTable 是使用 synchronize 关键字加锁的原理(就是对对象加锁); 

而针对 ConcurrentHashMap, 在 JDK 1.7 中采用 分段锁的方式; JDK 1.8 中直接采用了CAS(无锁算法)+ synchronized. 



**HashMap & ConcurrentHashMap 的区别?** 

除了加锁, 原理上无太大区别. 另外, HashMap 的键值对允许有 null, 但是 ConCurrentHashMap 都不允许. 



**为什么 ConcurrentHashMap 比 HashTable 效率要高**

HashTable 使用一把锁(锁住整个链表结构)处理并发问题, 多个线程竞争一把锁, 容易阻塞; 

ConcurrentHashMap

-   JDK 1.7 中使用分段锁(ReentrantLock + Segment + HashEntry), 相当于把一个 HashMap 分成多个段, 每段分配一把锁, 这样支持多线程访问. 锁粒度: 基于 Segment, 包含多个 HashEntry. 
-   JDK 1.8 中使用 CAS + synchronized + Node + 红黑树. 锁粒度: Node(首结点)(实现 Map.Entry). 锁粒度降低了. 



**ConcurrentHashMap 在 JDK 1.8 中, 为什么要使用内置锁 synchronized 来代替重入锁 ReentrantLock?** 

1.  粒度降低了

2.  JVM 开发团队没有放弃 synchronized, 而且基于 JVM 的 synchronized 优化空间更大, 更加自然. 

3.  在大量的数据操作下, 对于 JVM 的内存压力, 基于 API 的 ReentrantLock 会开销更多的内存. 



HashMap 遍历

HashMap **遍历从大的方向来说, 可分为以下 4 类**: 

1. 迭代器(Iterator)方式遍历; 
2. For Each 方式遍历; 
3. Lambda 表达式遍历(JDK 1.8+);
4. Streams API 遍历(JDK 1.8+). 

但每种类型下又有不同的实现方式, 因此具体的遍历方式又可以分为以下 7 种: 

1. 使用迭代器(Iterator)EntrySet 的方式进行遍历; 
2. 使用迭代器(Iterator)KeySet 的方式进行遍历; 
3. 使用 For Each EntrySet 的方式进行遍历; 
4. 使用 For Each KeySet 的方式进行遍历; 
5. 使用 Lambda 表达式的方式进行遍历; 
6. 使用 Streams API 单线程的方式进行遍历; 
7. 使用 Streams API 多线程的方式进行遍历. 

接下来我们来看每种遍历方式的具体实现代码. 

迭代器 EntrySet

```java
public class HashMapTest {
    public static void main(String[] args) {
        // 创建并赋值 HashMap
        Map<Integer, String> map = new HashMap();
        map.put(1, "Java");
        map.put(2, "JDK");
        map.put(3, "Spring Framework");
        map.put(4, "MyBatis framework");
        map.put(5, "Java中文社群");
        // 遍历
        Iterator<Map.Entry<Integer, String>> iterator = map.entrySet().iterator();
        while (iterator.hasNext()) {
            Map.Entry<Integer, String> entry = iterator.next();
            System.out.println(entry.getKey());
            System.out.println(entry.getValue());
        }
    }
}
```

迭代器 KeySet

```java
public class HashMapTest {
    public static void main(String[] args) {
        // 创建并赋值 HashMap
        Map<Integer, String> map = new HashMap();
        map.put(1, "Java");
        map.put(2, "JDK");
        map.put(3, "Spring Framework");
        map.put(4, "MyBatis framework");
        map.put(5, "Java中文社群");
        // 遍历
        Iterator<Integer> iterator = map.keySet().iterator();
        while (iterator.hasNext()) {
            Integer key = iterator.next();
            System.out.println(key);
            System.out.println(map.get(key));
        }
    }
}
```

ForEach EntrySet

```java
public class HashMapTest {
    public static void main(String[] args) {
        // 创建并赋值 HashMap
        Map<Integer, String> map = new HashMap();
        map.put(1, "Java");
        map.put(2, "JDK");
        map.put(3, "Spring Framework");
        map.put(4, "MyBatis framework");
        map.put(5, "Java中文社群");
        // 遍历
        for (Map.Entry<Integer, String> entry : map.entrySet()) {
            System.out.println(entry.getKey());
            System.out.println(entry.getValue());
        }
    }
}
```

ForEach KeySet

```java
public class HashMapTest {
    public static void main(String[] args) {
        // 创建并赋值 HashMap
        Map<Integer, String> map = new HashMap();
        map.put(1, "Java");
        map.put(2, "JDK");
        map.put(3, "Spring Framework");
        map.put(4, "MyBatis framework");
        map.put(5, "Java中文社群");
        // 遍历
        for (Integer key : map.keySet()) {
            System.out.println(key);
            System.out.println(map.get(key));
        }
    }
}
```

Lambda

```java
public class HashMapTest {
    public static void main(String[] args) {
        // 创建并赋值 HashMap
        Map<Integer, String> map = new HashMap();
        map.put(1, "Java");
        map.put(2, "JDK");
        map.put(3, "Spring Framework");
        map.put(4, "MyBatis framework");
        map.put(5, "Java中文社群");
        // 遍历
        map.forEach((key, value) -> {
            System.out.println(key);
            System.out.println(value);
        });
    }
}
```

Streams API 单线程

```java
public class HashMapTest {
    public static void main(String[] args) {
        // 创建并赋值 HashMap
        Map<Integer, String> map = new HashMap();
        map.put(1, "Java");
        map.put(2, "JDK");
        map.put(3, "Spring Framework");
        map.put(4, "MyBatis framework");
        map.put(5, "Java中文社群");
        // 遍历
        map.entrySet().stream().forEach((entry) -> {
            System.out.println(entry.getKey());
            System.out.println(entry.getValue());
        });
    }
}
```

Streams API 多线程

```java
public class HashMapTest {
    public static void main(String[] args) {
        // 创建并赋值 HashMap
        Map<Integer, String> map = new HashMap();
        map.put(1, "Java");
        map.put(2, "JDK");
        map.put(3, "Spring Framework");
        map.put(4, "MyBatis framework");
        map.put(5, "Java中文社群");
        // 遍历
        map.entrySet().parallelStream().forEach((entry) -> {
            System.out.println(entry.getKey());
            System.out.println(entry.getValue());
        });
    }
}
```



性能测试

接下来我们使用 Oracle 官方提供的性能测试工具 JMH(Java Microbenchmark Harness, JAVA 微基准测试套件)来测试一下这 7 种循环的性能. 

首先, 我们先要引入 JMH 框架, 在 `pom.xml` 文件中添加如下配置: 

```xml
<!-- https://mvnrepository.com/artifact/org.openjdk.jmh/jmh-core -->
<dependency>
    <groupId>org.openjdk.jmh</groupId>
    <artifactId>jmh-core</artifactId>
    <version>1.23</version>
</dependency>
<!-- https://mvnrepository.com/artifact/org.openjdk.jmh/jmh-generator-annprocess -->
<dependency>
    <groupId>org.openjdk.jmh</groupId>
    <artifactId>jmh-generator-annprocess</artifactId>
    <version>1.23</version>
    <scope>provided</scope>
</dependency>
```

然后编写测试代码, 如下所示: 

```java
@BenchmarkMode(Mode.AverageTime) // 测试完成时间
@OutputTimeUnit(TimeUnit.NANOSECONDS)
@Warmup(iterations = 2, time = 1, timeUnit = TimeUnit.SECONDS) // 预热 2 轮, 每次 1s
@Measurement(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS) // 测试 5 轮, 每次 1s
@Fork(1) // fork 1 个线程
@State(Scope.Thread) // 每个测试线程一个实例
public class HashMapCycleTest {

    static Map<Integer, String> map = new HashMap();

    static {
        // 添加数据
        for (int i = 0; i < 100; i++) {
            map.put(i, "val:" + i);
        }
    }

    public static void main(String[] args) throws RunnerException {
        // 启动基准测试
        Options opt = new OptionsBuilder().include(HashMapCycle.class.getSimpleName()) // 要导入的测试类
                .output("/Users/admin/Desktop/jmh-map.log") // 输出测试结果的文件
                .build();
        new Runner(opt).run(); // 执行测试
    }

    @Benchmark
    public void entrySet() {
        // 遍历
        Iterator<Map.Entry<Integer, String>> iterator = map.entrySet().iterator();
        while (iterator.hasNext()) {
            Map.Entry<Integer, String> entry = iterator.next();
            Integer k = entry.getKey();
            String v = entry.getValue();
        }
    }

    @Benchmark
    public void forEachEntrySet() {
        // 遍历
        for (Map.Entry<Integer, String> entry : map.entrySet()) {
            Integer k = entry.getKey();
            String v = entry.getValue();
        }
    }

    @Benchmark
    public void keySet() {
        // 遍历
        Iterator<Integer> iterator = map.keySet().iterator();
        while (iterator.hasNext()) {
            Integer k = iterator.next();
            String v = map.get(k);
        }
    }

    @Benchmark
    public void forEachKeySet() {
        // 遍历
        for (Integer key : map.keySet()) {
            Integer k = key;
            String v = map.get(k);
        }
    }

    @Benchmark
    public void lambda() {
        // 遍历
        map.forEach((key, value) -> {
            Integer k = key;
            String v = value;
        });
    }

    @Benchmark
    public void streamApi() {
        // 单线程遍历
        map.entrySet().stream().forEach((entry) -> {
            Integer k = entry.getKey();
            String v = entry.getValue();
        });
    }

    public void parallelStreamApi() {
        // 多线程遍历
        map.entrySet().parallelStream().forEach((entry) -> {
            Integer k = entry.getKey();
            String v = entry.getValue();
        });
    }

}
```

所有被添加了 `@Benchmark` 注解的方法都会被测试, 因为 parallelStream 为多线程版本性能一定是最好的, 所以就不参与测试了, 其他 6 个方法的测试结果如下: 

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_27.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_27.png" style="zoom:67%;" />

其中 Units 为 ns/op 意思是执行完成时间(单位为纳秒), 而 Score 列为平均执行时间,  `±` 符号表示误差. 从以上结果可以看出, 两个 `entrySet` 的性能相近, 并且执行速度最快, 接下来是 `stream` , 然后是两个 `keySet`, 性能最差的是 `KeySet` . 

> 注: 以上结果基于测试环境: JDK 1.8 / Mac mini (2018) / Idea 2020.1





结论

**从以上结果可以看出 `entrySet` 的性能比 `keySet` 的性能高出了一倍之多, 因此我们应该尽量使用 `entrySet` 来实现 Map 集合的遍历**. 

字节码分析

要理解以上的测试结果, 我们需要把所有遍历代码通过 `javac` 编译成字节码来看具体的原因. 

编译后, 我们使用 Idea 打开字节码, 内容如下: 

```java
//
// Source code recreated from a .class file by IntelliJ IDEA
// (powered by Fernflower decompiler)
//

package com.example;

import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.util.Map.Entry;

public class HashMapTest {
    static Map<Integer, String> map = new HashMap() {
        {
            for(int var1 = 0; var1 < 2; ++var1) {
                this.put(var1, "val:" + var1);
            }

        }
    };

    public HashMapTest() {
    }

    public static void main(String[] var0) {
        entrySet();
        keySet();
        forEachEntrySet();
        forEachKeySet();
        lambda();
        streamApi();
        parallelStreamApi();
    }

    public static void entrySet() {
        Iterator var0 = map.entrySet().iterator();

        while(var0.hasNext()) {
            Entry var1 = (Entry)var0.next();
            System.out.println(var1.getKey());
            System.out.println((String)var1.getValue());
        }

    }

    public static void keySet() {
        Iterator var0 = map.keySet().iterator();

        while(var0.hasNext()) {
            Integer var1 = (Integer)var0.next();
            System.out.println(var1);
            System.out.println((String)map.get(var1));
        }

    }

    public static void forEachEntrySet() {
        Iterator var0 = map.entrySet().iterator();

        while(var0.hasNext()) {
            Entry var1 = (Entry)var0.next();
            System.out.println(var1.getKey());
            System.out.println((String)var1.getValue());
        }

    }

    public static void forEachKeySet() {
        Iterator var0 = map.keySet().iterator();

        while(var0.hasNext()) {
            Integer var1 = (Integer)var0.next();
            System.out.println(var1);
            System.out.println((String)map.get(var1));
        }

    }

    public static void lambda() {
        map.forEach((var0, var1) -> {
            System.out.println(var0);
            System.out.println(var1);
        });
    }

    public static void streamApi() {
        map.entrySet().stream().forEach((var0) -> {
            System.out.println(var0.getKey());
            System.out.println((String)var0.getValue());
        });
    }

    public static void parallelStreamApi() {
        map.entrySet().parallelStream().forEach((var0) -> {
            System.out.println(var0.getKey());
            System.out.println((String)var0.getValue());
        });
    }
}

```

从结果可以看出, 除了 Lambda 和 Streams API 之外, 通过迭代器循环和 `for` 循环的遍历的 `EntrySet` 最终生成的代码是一样的, 他们都是在循环中创建了一个遍历对象 `Entry` , 代码如下: 

```java
public static void entrySet() {
    Iterator var0 = map.entrySet().iterator();
    while(var0.hasNext()) {
        Entry var1 = (Entry)var0.next();
        System.out.println(var1.getKey());
        System.out.println((String)var1.getValue());
    }
}
public static void forEachEntrySet() {
    Iterator var0 = map.entrySet().iterator();
    while(var0.hasNext()) {
        Entry var1 = (Entry)var0.next();
        System.out.println(var1.getKey());
        System.out.println((String)var1.getValue());
    }
}
```

而 `KeySet` 的代码也是类似的, 如下所示: 

```java
public static void keySet() {
    Iterator var0 = map.keySet().iterator();
    while(var0.hasNext()) {
        Integer var1 = (Integer)var0.next();
        System.out.println(var1);
        System.out.println((String)map.get(var1));
    }
} 
public static void forEachKeySet() {
    Iterator var0 = map.keySet().iterator();
    while(var0.hasNext()) {
        Integer var1 = (Integer)var0.next();
        System.out.println(var1);
        System.out.println((String)map.get(var1));
    }
}
```

所以我们在使用迭代器或是 `for` 循环 `EntrySet` 时, 他们的性能都是相同的, 因为他们最终生成的字节码基本都是一样的; 同理 `KeySet` 的两种遍历方式也是类似的. 



性能分析

`EntrySet` 之所以比 `KeySet` 的性能高是因为, `KeySet` 在循环时使用了 `map.get(key)`, 而 `map.get(key)` 相当于又遍历了一遍 Map 集合去查询 `key` 所对应的值. 为什么要用“又”这个词？那是因为**在使用迭代器或者 for 循环时, 其实已经遍历了一遍 Map 集合了, 因此再使用 `map.get(key)` 查询时, 相当于遍历了两遍**. 

而 `EntrySet` 只遍历了一遍 Map 集合, 之后通过代码“Entry<Integer, String> entry = iterator.next()”把对象的 `key` 和 `value` 值都放入到了 `Entry` 对象中, 因此再获取 `key` 和 `value` 值时就无需再遍历 Map 集合, 只需要从 `Entry` 对象中取值就可以了. 

所以, **`EntrySet` 的性能比 `KeySet` 的性能高出了一倍, 因为 `KeySet` 相当于循环了两遍 Map 集合, 而 `EntrySet` 只循环了一遍**. 



安全性测试

从上面的性能测试结果和原理分析, 我想大家应该选用那种遍历方式, 已经心中有数的, 而接下来我们就从「安全」的角度入手, 来分析那种遍历方式更安全. 

我们把以上遍历划分为四类进行测试: 迭代器方式, For 循环方式, Lambda 方式和 Stream 方式, 测试代码如下. 

1.迭代器方式

```java
Iterator<Map.Entry<Integer, String>> iterator = map.entrySet().iterator();
while (iterator.hasNext()) {
    Map.Entry<Integer, String> entry = iterator.next();
    if (entry.getKey() == 1) {
        // 删除
        System.out.println("del:" + entry.getKey());
        iterator.remove();
    } else {
        System.out.println("show:" + entry.getKey());
    }
}
```

以上程序的执行结果: 

> show:0
>
> del:1
>
> show:2

测试结果: **迭代器中循环删除数据安全**. 



2.For 循环方式

```java
for (Map.Entry<Integer, String> entry : map.entrySet()) {
    if (entry.getKey() == 1) {
        // 删除
        System.out.println("del:" + entry.getKey());
        map.remove(entry.getKey());
    } else {
        System.out.println("show:" + entry.getKey());
    }
}
```

以上程序的执行结果: 

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_28.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_28.png" style="zoom:67%;" />

测试结果: **For 循环中删除数据非安全**. 

3.Lambda 方式

```java
map.forEach((key, value) -> {
    if (key == 1) {
        System.out.println("del:" + key);
        map.remove(key);
    } else {
        System.out.println("show:" + key);
    }
});
```

以上程序的执行结果: 

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_29.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_29.png" style="zoom:67%;" />

测试结果: **Lambda 循环中删除数据非安全**. 

**Lambda 删除的正确方式**: 

```java
// 根据 map 中的 key 去判断删除
map.keySet().removeIf(key -> key == 1);
map.forEach((key, value) -> {
    System.out.println("show:" + key);
});
```

以上程序的执行结果: 

> show:0
>
> show:2

从上面的代码可以看出, 可以先使用 `Lambda` 的 `removeIf` 删除多余的数据, 再进行循环是一种正确操作集合的方式. 

4.Stream 方式

```java
map.entrySet().stream().forEach((entry) -> {
    if (entry.getKey() == 1) {
        System.out.println("del:" + entry.getKey());
        map.remove(entry.getKey());
    } else {
        System.out.println("show:" + entry.getKey());
    }
});
```

以上程序的执行结果: 

<img src="http://www.milky.show/images/java/collection/hashmap/hashmap_30.png" alt="http://www.milky.show/images/java/collection/hashmap/hashmap_30.png" style="zoom:67%;" />

测试结果: **Stream 循环中删除数据非安全**. 

**Stream 循环的正确方式**: 

```java
map.entrySet().stream().filter(m -> 1 != m.getKey()).forEach((entry) -> {
    if (entry.getKey() == 1) {
        System.out.println("del:" + entry.getKey());
    } else {
        System.out.println("show:" + entry.getKey());
    }
});
```

以上程序的执行结果: 

> show:0
>
> show:2

从上面的代码可以看出, 可以使用 `Stream` 中的 `filter` 过滤掉无用的数据, 再进行遍历也是一种安全的操作集合的方式. 

小结

我们不能在遍历中使用集合 `map.remove()` 来删除数据, 这是非安全的操作方式, 但我们可以使用迭代器的 `iterator.remove()` 的方法来删除数据, 这是安全的删除集合的方式. 同样的我们也可以使用 Lambda 中的 `removeIf` 来提前删除数据, 或者是使用 Stream 中的 `filter` 过滤掉要删除的数据进行循环, 这样都是安全的, 当然我们也可以在 `for` 循环前删除数据在遍历也是线程安全的. 

总结

本文我们讲了 HashMap 4 种遍历方式: 迭代器, for, lambda, stream, 以及具体的 7 种遍历方法, 综合性能和安全性来看, **我们应该尽量使用迭代器(Iterator)来遍历 `EntrySet` 的遍历方式来操作 Map 集合**, 这样就会既安全又高效了. 







## ConcurrentHashMap

简单来说, HashMap 是一个 Entry 对象的数组.数组中的每一个 Entry 元素, 又是一个链表的头节点.

**Hashmap 不是线程安全的.在高并发环境下做插入操作, 有可能出现环型列表导致下一次查询死循环, 想要避免线程安全问题有很多办法, 比如改用 HashTable 或者 Collections.synchronizedMap 方法, 但是这两者有着共同的问题, 性能非常低下, 无论读操作还是写操作, 它们都会给整个集合加锁, 导致同一时间的其他操作阻塞**

在并发环境下, 如何能够兼顾线程安全和运行效率呢, 这时候 ConcurrentHashMap 就应运而生了

**ConcurrentHashMap 其实很简单, 最关键是要理解一个概念: Segment**

同 HashMap 一样, Segment 包含一个 HashEntry 数组, 数组中的每一个 HashEntry 既是一个键值对, 也是一个链表的头节点.

单一的 Segment 结构如下: 

<img src="http://www.milky.show/images/java/collection/chm/ch_1.png" alt="http://www.milky.show/images/java/collection/chm/ch_1.png" style="zoom:67%;" />

**像这样的 Segment 对象, 在 ConcurrentHashMap 集合中有多少个呢?有 2 的 N 次方个, 共同保存在一个名为 segments 的数组当中.**

因此整个 ConcurrentHashMap 的结构如下: 

<img src="http://www.milky.show/images/java/collection/chm/ch_2.png" alt="http://www.milky.show/images/java/collection/chm/ch_2.png" style="zoom:67%;" />

**可以说, ConcurrentHashMap 是一个二级哈希表. 在一个总的哈希表下面, 有若干个子哈希表.**

**这样的二级结构, 和数据库的水平拆分有些相似.**

ConcurrentHashMap 的优势就是采用了**锁分段技术**, 每一个 Segment 就好比一个自治区, 读写操作高度自治, Segment 之间互不影响

**Case1: 不同 Segment 的并发写入**

<img src="http://www.milky.show/images/java/collection/chm/ch_3.png" alt="http://www.milky.show/images/java/collection/chm/ch_3.png" style="zoom:67%;" />

不同Segment的写入是可以并发执行的.

**Case2: 同一 Segment 的一写一读**

<img src="http://www.milky.show/images/java/collection/chm/ch_4.png" alt="http://www.milky.show/images/java/collection/chm/ch_4.png" style="zoom:67%;" />

**Case3: 同一  Segment 的并发写入**

<img src="http://www.milky.show/images/java/collection/chm/ch_5.png" alt="http://www.milky.show/images/java/collection/chm/ch_5.png" style="zoom:67%;" />

**Segment 的写入是需要上锁的, 因此对同一 Segment 的并发写入会被阻塞.**

**由此可见, ConcurrentHashMap 当中每个 Segment 各自持有一把锁.在保证线程安全的同时降低了锁的粒度, 让并发操作效率更高.**

**Get 方法**

1. 为输入的 Key 做 Hash 运算, 得到 hash 值(Hash 的过程实际上是 2 次保证平均)

2. 通过 hash 值, 定位到对应的 Segment 对象

3. 再次通过 hash 值, 定位到 Segment 当中数组的具体位置.

**PUT 方法**

1. 为输入的 Key 做 Hash 运算, 得到 hash 值

2. 通过 hash 值, 定位到对应的 Segment 对象

3. 获取可重入锁

4. 再次通过 hash 值, 定位到 Segment 当中数组的具体位置

5. 插入或覆盖 HashEntry 对象

6. 释放锁

从步骤可以看出, ConcurrentHashMap 在读写时都需要二次定位. 首先定位到 Segment, 之后定位到 Segment 内的具体数组下标

**size 方法如何保证一致性**

每个 segment 都各自加锁, 在调用 size 方法时怎么解决一致性问题呢

Size 方法的目的是统计 ConcurrentHashMap 的总元素数量,  自然需要把各个 Segment 内部的元素数量汇总起来.

**但是, 如果在统计 Segment 元素数量的过程中, 已统计过的 Segment 瞬间插入新的元素, 这时候该怎么办呢?**

<img src="http://www.milky.show/images/java/collection/chm/ch_6.png" alt="http://www.milky.show/images/java/collection/chm/ch_6.png" style="zoom:67%;" />

<img src="http://www.milky.show/images/java/collection/chm/ch_7.png" alt="http://www.milky.show/images/java/collection/chm/ch_7.png" style="zoom:67%;" />

<img src="http://www.milky.show/images/java/collection/chm/ch_8.png" alt="http://www.milky.show/images/java/collection/chm/ch_8.png" style="zoom:67%;" />

**ConcurrentHashMap 的 Size 方法是一个嵌套循环, 大体逻辑如下**

1. 遍历所有的 Segment.

2. 把 Segment 的元素数量累加起来.

3. 把 Segment 的修改次数累加起来.

4. 判断所有 Segment 的总修改次数是否大于上一次的总修改次数.如果大于, 说明统计过程中有修改, 重新统计, 尝试次数+1, 如果不是.说明没有修改, 统计结束.

5. 如果尝试次数超过阈值, 则对每一个 Segment 加锁, 再重新统计.

6. 再次判断所有 Segment 的总修改次数是否大于上一次的总修改次数.由于已经加锁, 次数一定和上次相等.

7. 释放锁, 统计结束.

**为什么这样设计呢? 这种思想和乐观锁悲观锁的思想如出一辙.**

**为了尽量不锁住所有 Segment, 首先乐观地假设 Size 过程中不会有修改.当尝试一定次数, 才无奈转为悲观锁, 锁住所有Segment 保证强一致性.**



**ConcurrentHashMap 的并发度是什么**

程序运行时能够同时更新 ConccurentHashMap 且不产生锁竞争的最大线程数. 默认为 16, 且可以在构造函数中设置. 

当用户设置并发度时, ConcurrentHashMap 会使用大于等于该值的最小2幂指数作为实际并发度(假如用户设置并发度为17, 实际并发度则为32)



**在JDK8中,  ConcurrentHashMap的内部实现发生了天翻地覆的变化. 这里依据JDK8, 来介绍一下ConcurrentHashMap的内部实现**

从静态数据结构上说, ConcurrentHashMap包含以下内容: 

**int sizeCtl**

这是一个多功能的字段, 可以用来记录参与Map扩展的线程数量, 也用来记录新的table的扩容阈值

**CounterCell[] counterCells**

用来记录元素的个数, 这是一个数组, 使用数组来记录, 是因为避免多线程竞争时, 可能产生的冲突. 使用了数组, 那么多个线程同时修改数量时, 极有可能实际操作数组中不同的单元, 从而减少竞争. 

**Node<K,V>[] table**

实际存放Map内容的地方, 一个map实际上就是一个Node数组, 每个Node里包含了key和value的信息. 

**Node<K,V>[] nextTable**

当table需要扩充时, 会把新的数据填充到nextTable中, 也就是说nextTable是扩充后的Map. 

以上就是ConcurrentHashMap的核心元素, 其中最值得注意的便是Node, Node并非想象中如此简单, 下面的图展示了Node的类族结构: 

<img src="http://www.milky.show/images/java/collection/chm/ch_9.png" alt="http://www.milky.show/images/java/collection/chm/ch_9.png" style="zoom:67%;" />

可以看到, 在Map中的Node并非简单的Node对象, 实际上, 它有可能是Node对象, 也有可能是一个Treebin或者ForwardingNode. 

那什么时候是Node, 什么时候是TreeBin, 什么时候又是一个ForwardingNode呢？

其实在绝大部分场景中, 使用的依然是Node, 从Node数据结构中, 不难看出, Node其实是一个链表, 也就是说, 一个正常的Map可能是长这样的: 

<img src="http://www.milky.show/images/java/collection/chm/ch_10.png" alt="http://www.milky.show/images/java/collection/chm/ch_10.png" style="zoom:67%;" />

上图中, 绿色部分表示 Node 数组, 里面的元素是 Node, 也就是链表的头部, 当两个元素在数据中的位置发生冲突时, 就将它们通过链表的形式, 放在一个槽位中. 

当数组槽位对应的是一个链表时, 在一个链表中查找 key 只能使用简单的遍历, 这在数据不多时, 还是可以接受的, 当冲突数据比较多少, 这种简单的遍历就有点慢了. 

因此, 在具体实现中, 当链表的长度大于等于 8 时, 会将链表树状化, 也就是变成一颗红黑树. 如下图所示, 其中一个槽位就变成了一颗树, 这就是 TreeBin(在TreeBin 中使用 TreeNode 构造整科树). 

<img src="http://www.milky.show/images/java/collection/chm/ch_11.png" alt="http://www.milky.show/images/java/collection/chm/ch_11.png" style="zoom:67%;" />

当数组容量快满时, 即超过 75% 的容量时, 数组还需要进行扩容, 在扩容过程中, 如果老的数组已经完成了复制, 那么就会将老数组中的元素使用ForwardingNode 对象替代, 表示当前槽位的数据已经处理了, 不需要再处理了, 这样, 当有多个线程同时参与扩容时, 就不会冲突. 

**put() 方法的实现**

现在来看一下作为一个 HashMap 最为重要的方法 put()

- public V put(K key, V value)

它负责将给定的 key 和 value 对存入 HashMap, 它的工作主要有以下几个步骤: 

1. 如果没有初始化数组, 则尝试初始化数组
2. 如果当前正在扩容, 则参与帮助扩容(调用helpTransfer()方法)
3. 将给定的 key, value 放入对应的槽位
4. 统计元素总数
5. 触发扩容操作

根据以上主要 4 个步骤, 来依次详细说明一下: 

**如果没有初始化数组, 则尝试初始化数组**

初始化数据会生成一个 Node 数组:

```java
Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];
```

默认情况下, n 为 16. 同时设置 sizeCtl 为·`n - (n >>> 2)`; 这意味着 sizeCtl 为 n 的 75%, 表示 Map 的 size, 也就是说 ConcurrentHashMap 的负载因子是0.75. (为了避免冲突, Map 的容量是数组的 75%, 超过这个阈值, 就会扩容)

**如果当前正在扩容, 则参与帮助扩容**

```java
else if ((fh = f.hash) == MOVED)
    tab = helpTransfer(tab, f);
```

如果一个节点的 hash 是 MOVE, 则表示这是一个 ForwardingNode, 也就是当前正在扩容中, 为了尽快完成扩容, 当前线程就会参与到扩容的工作中, 而不是等待扩容操作完成, 如此紧密细致的操作, 恰恰是 ConcurrentHashMap 高性能的原因. 

而代码中的`f.hash==MOVE`语义上等同于`f instanceof ForwardingNode`, 但是使用整数相等的判断的效率要远远高于 instanceof, 所以, 这里也是一处对性能的极限优化. 

**将给定的 key, value 放入对应的槽位**

在大部分情况下, 应该会走到这一步, 也就是将 key 和 value 放入数组中. 在这个操作中会使用大概如下操作: 

```java
Node<K,V> f;
synchronized (f) {
		if(所在槽位是一个链表)
				插入链表
		else if(所在槽位是红黑树)
				插入树
		if(链表长度大于8[TREEIFY_THRESHOLD])
				将链表树状化
}
```

可以看到, 这使用了 synchronized 关键字, 锁住了 Node 对象. 由于在绝大部分情况下, 不同线程大概率会操作不同的 Node, 因此这里的竞争应该不会太大. 

并且随着数组规模越来越大, 竞争的概率会越来越小, 因此 ConcurrentHashMap 有了极好的并行性. 

**统计元素总数**

为了有一个高性能的 size() 方法, ConcurrentHashMap 使用了单独的方法来统计元素总数, 元素数量统计在 CounterCell 数组中: 

```java
CounterCell[] counterCells;
@sun.misc.Contended static final class CounterCell {
    volatile long value;
    CounterCell(long x) { value = x; }
}
```

CounterCell 使用伪共享优化, 具有很高的读写性能. counterCells 中所有的成员的 value 相加, 就是整个 Map 的大小. 这里使用数组, 也是为了防止冲突. 

如果简单使用一个变量, 那么多线程累加一个计数器时, 难免要有竞争, 现在分散到一个数组中, 这种竞争就小了很多, 对并发就更加友好了. 

累加的主要逻辑如下: 

```java
if (as == null || (m = as.length - 1) < 0 ||
    // 不同线程映射到不同的数组元素, 防止冲突
    (a = as[ThreadLocalRandom.getProbe() & m]) == null ||
    // 使用CAS直接增加对应的数据
    !(uncontended =
      U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x)))
    // 如果有竞争, 在这里会重试, 如果竞争严重还会将CounterCell[]数组扩容, 以减少竞争
```

**触发扩容操作**

最后, ConcurrentHashMap 还会检查是否需要扩容, 它会检查当前Map的大小是否超过了阈值, 如果超过了, 还会进行扩容. 

ConcurrentHashMap 的扩容过程非常巧妙, 它并没有完全打乱当前已有的元素位置, 而是在数组扩容 2 倍后, 将一半的元素移动到新的空间中. 

所有的元素根据高位是否为 1 分为 low 节点和 high 节点: 

```java
// n 是数组长度, 数组长度是 2 的幂次方, 因此一定是 100 1000 10000 100000 这种二进制数字
// 这里将 low 节点串一起, high 节点串一起
if ((ph & n) == 0)
    ln = new Node<K,V>(ph, pk, pv, ln);
else
    hn = new Node<K,V>(ph, pk, pv, hn);
```

接着, 重新放置这些元素的位置: 

```java
// low节点留在当前位置
setTabAt(nextTab, i, ln);
// high节点放到扩容后的新位置, 新位置距离老位置n
setTabAt(nextTab, i + n, hn);
// 扩容完成, 用ForwardingNode填充
setTabAt(tab, i, fwd);
```

下图显示了从 8 扩充到 16 时的可能得一种扩容情况, 注意, 新的位置总是在老位置的后面n个槽位(n 为原数组大小)

<img src="http://www.milky.show/images/java/collection/chm/ch_12.png" alt="http://www.milky.show/images/java/collection/chm/ch_12.png" style="zoom: 50%;" />

这样做的好处是, 每个元素的位置不需要重新计算, 进行查找时, 由于总是会对 n-1(一定是一个类似于1111 11111 111111这样的二进制数)按位与, 因此, high 类的节点自然就会出现在 +n 的位置上. 

**get() 方法的实现**

与 put() 方法相比, get() 方法就比较简单了. 步骤如下: 

1. 根据hash值 得到对应的槽位 (n - 1) & h

2. 如果当前槽位第一个元素key就和请求的一样, 直接返回

3. 否则调用 Node 的 find() 方法查找

4. 对于 ForwardingNode  使用的是 ForwardingNode.find()

5. 对于红黑树 使用的是 TreeBin.find()

6. 对于链表型的槽位, 依次顺序查找对应的 key



**为什么要使用 CAS+Synchronized 取代 Segment+ReentrantLock**

锁的粒度降低了, ReentrantLock 的 segment 是固定的, 每段内是一个数组, 而 cas 是头结点锁, 提高了并发度那么竞争会变低, 而且 synchronized 有锁升级的优化



**1.7 与 1.8 的区别**

数据结构

*   1.7 采用 Segment + 链表
*   1.8 采用 Node + 链表 + 红黑树

Hash 碰撞

*   1.7 会一直使用链表
*   1.8 首先使用链表, 当链表长度为 8 时转化为红黑树

保证并发安全

*   1.7 采用分段锁 Segment
*   1.8 采用 CAS + Synchronized

查询复杂度

*   1.7 是遍历链表O(n)
*   1.8 是遍历链表 O(n) + 遍历红黑树 O(logN)



**ConcurrentHashMap 也不是线程安全的?**

ConcurrentHashMap 本身肯定是线程安全的, 但是在组合操作下并不保证线程安全, 作者考虑到这种情况提供了 replace() 方法



## List

**常见 List 初始化方式**

1. **先创建 List 在赋值**

    标准方式, 先创建集合对象, 然后逐个调用`add`方法初始化. 用起来比较繁琐, 不太方便

    ```java
    List<Integer> list = new ArrayList<>();
    list.add(1);
    list.add(2);
    list.add(3);
    ```

2. **使用`{{}}`双大括号初始化**

    使用匿名内部类完成初始化. 外层的`{}`定义了一个ArrayList的匿名内部类, 内层的`{}`定义了一个实例初始化的非静态构造代码块. **有内存泄露风险**

    ```java
    List<Integer> list = new ArrayList(){
        {
            add(1);
            add(2);
            add(3);
        }
    }
    ```

3. **使用 Arrays.asList**

    使用 Arrays 的静态方法 `asList` 初始化. **返回的 list 集合是不可变的**

    ```java
    List<Integer> list = Arrays.asList(1, 2, 3);
    ```

4. **使用 Stream (JDK8 以上)**

    使用 JDK8 引入的 Stream 的 of 方法生成一个 stream 对象, 调用 `collect` 方法进行收集, 形成一个 List 集合

    ```java
    List<Integer> list = Stream.of(1, 2, 3).collect(Collectors.toList());
    ```

5. **使用 Google Guava(需要引入 Guava 工具包)**

    借助 Google Guava 工具集中的 `Lists` 工具类初始化. 需要引入 Guava 才能使用

    ```java
    List<Integer> list = Lists.newArrayList(1, 2, 3);
    ```

6. **使用 Lists(JDK9 以上)**

    使用 JDK9 引入的 `Lists` 完成初始化

    ```java
    List<Integer> list = Lists.of(1, 2, 3);
    ```



**List 去重**

1. 使用 LinkedHashSet 删除 arraylist 中的重复数据

    LinkedHashSet 是在一个 ArrayList 删除重复数据的最佳方法. LinkedHashSet 在内部完成两件事

    - 删除重复数据
    - 保持添加到其中的数据的顺序

    

    Java 示例使用 LinkedHashSet 删除 arraylist 中的重复项. 在给定的示例中, numbersList 是包含整数的 arraylist, 其中一些是重复的数字. 

    例如 1,3 和 5. 我们将列表添加到 LinkedHashSet, 然后将内容返回到列表中. 结果 arraylist 没有重复的整数. 

    ```java
    import java.util.ArrayList;
    import java.util.Arrays;
    import java.util.LinkedHashSet;
     
    public class ArrayListExample {
        public static void main(String[] args) {
    
            ArrayList<Integer> numbersList = new ArrayList<>(Arrays.asList(1, 1, 2, 3, 3, 3, 4, 5, 6, 6, 6, 7, 8));
     
            System.out.println(numbersList);
     
            LinkedHashSet<Integer> hashSet = new LinkedHashSet<>(numbersList);
     
            ArrayList<Integer> listWithoutDuplicates = new ArrayList<>(hashSet);
     
            System.out.println(listWithoutDuplicates);
     
        } 
    }
    ```

    输出结果

    ```java
    [1, 1, 2, 3, 3, 3, 4, 5, 6, 6, 6, 7, 8]
     
    [1, 2, 3, 4, 5, 6, 7, 8]
    ```

2. 使用 java8 新特性 stream 进行 List 去重

    要从 arraylist 中删除重复项, 我们也可以使用 java 8 stream api. 使用 steam 的 distinct() 方法返回一个由不同数据组成的流, 通过对象的equals()方法进行比较. 

    收集所有区域数据 List 使用 Collectors.toList(). 

    Java 程序, 用于在不使用 Set 的情况下从 java 中的 arraylist 中删除重复项. 

    ```java
    import java.util.ArrayList;
    import java.util.Arrays;
    import java.util.List;
    import java.util.stream.Collectors;
     
    public class ArrayListExample {
        public static void main(String[] args) {
    
            ArrayList<Integer> numbersList = new ArrayList<>(Arrays.asList(1, 1, 2, 3, 3, 3, 4, 5, 6, 6, 6, 7, 8));
            System.out.println(numbersList);
            List<Integer> listWithoutDuplicates = numbersList.stream().distinct().collect(Collectors.toList());
     
            System.out.println(listWithoutDuplicates);
     
        }
     
    }
    ```

    输出结果

    ```java
    [1, 1, 2, 3, 3, 3, 4, 5, 6, 6, 6, 7, 8]
     
    [1, 2, 3, 4, 5, 6, 7, 8]
    ```

3. 利用 HashSet 不能添加重复数据的特性 由于 HashSet 不能保证添加顺序, 所以只能作为判断条件保证顺序

    ```java
    private static void removeDuplicate(List<String> list) {
        HashSet<String> set = new HashSet<String>(list.size());
        List<String> result = new ArrayList<String>(list.size());
        for (String str : list) {
            if (set.add(str)) {
                result.add(str);
            }
        }
        list.clear();
        list.addAll(result);
    }
    ```

4. 利用 List 的 contains 方法循环遍历,重新排序, 只添加一次数据, 避免重复

    ```java
    private static void removeDuplicate(List<String> list) {
        List<String> result = new ArrayList<String>(list.size());
        for (String str : list) {
            if (!result.contains(str)) {
                result.add(str);
            }
        }
        list.clear();
        list.addAll(result);
    }
    ```

5. 双重 for 循环去重

    ```java
    for (int i = 0; i < list.size(); i++) {
        for (int j = 0; j < list.size(); j++) {
            if(i!=j&&list.get(i)==list.get(j)) {
                list.remove(list.get(j));
            }
        }
    }
    ```

    





## 数组与集合的转换

**数组转 List**

```java
String[] staffs = new String[]{"A", "B", "C"};
List staffsList = Arrays.asList(staffs);

// 注意: Arrays.asList() 返回一个受指定数组决定的固定大小的列表. 所以不能做 add, remove 等操作, 否则会报错. 
List staffsList1 = Arrays.asList(staffs);
staffsList1.add("D"); // UnsupportedOperationException
staffsList1.remove(0); // UnsupportedOperationException

// 以下方式可以进行增删操作. 
List staffsList2 = new ArrayList<String>();
for (String temp : staffs) { 
    staffsList2.add(temp);
}
staffsList2.add("D"); // ok
staffsList2.remove(0); // ok
```

**数组转 Set**

```java
String[] staffs = new String[]{"A", "B", "C"};
Set<String> staffsSet = new HashSet<>(Arrays.asList(staffs));
staffsSet.add("D"); // ok
staffsSet.remove("Tom"); // ok
```

**List 转数组**

```java
String[] staffs = new String[]{"A", "B", "C"};
List staffsList = Arrays.asList(staffs);

Object[] result = staffsList.toArray();
```

**List 转 Set**

```java
String[] staffs = new String[]{"A", "B", "C"};
List staffsList = Arrays.asList(staffs);

Set result = new HashSet(staffsList);
```

**Set 转数组**

```java
String[] staffs = new String[]{"A", "B", "C"};
Set<String> staffsSet = new HashSet<>(Arrays.asList(staffs));

Object[] result = staffsSet.toArray();
```

**Set 转 List**

```java
String[] staffs = new String[]{"A", "B", "C"};
Set<String> staffsSet = new HashSet<>(Arrays.asList(staffs));

List<String> result = new ArrayList<>(staffsSet);
```

 