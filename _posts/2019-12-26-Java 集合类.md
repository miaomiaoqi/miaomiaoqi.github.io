---
layout: post
title: "Java 集合类"
categories: [Java]
description:
keywords:
---

* content
{:toc}


## HashMap

众所周知, HashMap是一个用于存储Key-Value键值对的集合, 每一个键值对也叫做**Entry**.这些个键值对(Entry)分散存储在一个数组当中, 这个数组就是HashMap的主干.

HashMap数组每一个元素的初始值都是Null.

![http://www.milky.show/images/java/hashmap/hashmap_1.png](http://www.milky.show/images/java/hashmap/hashmap_1.png)

对于HashMap, 我们最常使用的是两个方法: **Get** 和 **Put**.**利用 hashCode 方法找到链表, 利用 equals 方法找到重复的 key**

### Put 方法的原理

调用Put方法的时候发生了什么呢?

比如调用 hashMap.put("apple", 0), 插入一个Key为“apple"的元素.这时候我们需要利用一个哈希函数来确定 Entry 的插入位置(index): 

**index = Hash(“apple”)**

假定最后计算出的 index 是 2, 那么结果如下: 

![http://www.milky.show/images/java/hashmap/hashmap_2.png](http://www.milky.show/images/java/hashmap/hashmap_2.png)

**但是, 因为HashMap的长度是有限的, 当插入的Entry越来越多时, 再完美的Hash函数也难免会出现 index 冲突的情况.比如下面这样: **

![http://www.milky.show/images/java/hashmap/hashmap_3.png](http://www.milky.show/images/java/hashmap/hashmap_3.png)

这时候该怎么办呢?我们可以利用**链表**来解决.

HashMap数组的每一个元素不止是一个Entry对象, 也是一个链表的头节点.每一个Entry对象通过Next指针指向它的下一个Entry节点.当新来的Entry映射到冲突的数组位置时, 只需要插入到对应的链表即可: 

![http://www.milky.show/images/java/hashmap/hashmap_4.png](http://www.milky.show/images/java/hashmap/hashmap_4.png)

**在链表中插入的时候会调用 equals 方法比较两个 key 是否为同一个元素, 如果相等会覆盖原有的数据**

**需要注意的是, 新来的Entry节点插入链表时, 使用的是“头插法”.至于为什么不插入链表尾部, 后面会有解释.**



### Get 方法的原理

使用Get方法根据Key来查找Value的时候, 发生了什么呢?

首先会把输入的Key做一次Hash映射, 得到对应的index: 

**index = Hash(“apple”)**

由于刚才所说的Hash冲突, 同一个位置有可能匹配到多个Entry, 这时候就需要顺着对应链表的头节点, 一个一个向下来查找.假设我们要查找的Key是“apple”: 

![http://www.milky.show/images/java/hashmap/hashmap_5.png](http://www.milky.show/images/java/hashmap/hashmap_5.png)

第一步, 我们查看的是头节点Entry6, Entry6的Key是banana, 显然不是我们要找的结果.

第二步, 我们查看的是Next节点Entry1, Entry1的Key是apple, 正是我们要找的结果.

**之所以把Entry6放在头节点, 是因为HashMap的发明者认为, 后插入的Entry被查找的可能性更大**.



### HashMap 的初始长度

**HashMap 的默认初始长度是 16, 并且每次自动扩展或是手动初始化时, 长度必须是 2 的幂**

**之所以选择 16, 是为了服务于从 Key 映射到 index 的 hash 算法**

**index = Hash(“apple”)**

如何实现一个尽量均匀分布的Hash函数呢?我们通过利用 Key 的 HashCode 值来做某种运算.

**index = HashCode(Key) % Length**, 这种取模运算很简单, 但是效率很低. 为了实现高效的 Hash 算法, HashMap 发明者采用了位运算的方式

如何进行位运算呢?有如下的公式(Length 是 HashMap的长度): 

**index = HashCode(Key) & (Length - 1)** 

下面我们以值为 “book” 的 Key 来演示整个过程: 

1. 计算 book 的 hashcode, 结果为十进制的 3029737, 二进制的 101110001110101110 1001
2. 假定HashMap长度是默认的 16, 计算 Length - 1 的结果为十进制的 15, 二进制的 1111

3. 把以上两个结果做**与运算**, 101110001110101110 1001 & 1111 = 1001, 十进制是 9, 所以 index = 9

可以说, Hash 算法最终得到的 index 结果, 完全取决于 Key 的 Hashcode 值的最后几位

**这样做的效果上等同于取模, 而且还大大提高了性能, 至于为什么采用 16, 我们可以试试长度是 10会出现什么问题**

假设HashMap的长度是 10, 重复刚才的运算步骤: 

1. 计算 book 的 hashcode, 结果为十进制的 3029737, 二进制的 101110001110101110 1001
2. HashMap 的长度是 10, 计算 Length - 1 的结果为十进制的 9, 二进制 1001
3. 把以上两个结果做**与运算**, 101110001110101110 1001 & 1001 = 1001, 十进制是 9, 所以 index = 9

单独看这个结果, 表面上并没有问题.我们再来尝试一个新的HashCode 101110001110101110 **1011:** 结果为 1001

让我们再换一个HashCode 101110001110101110 **1111:**  结果为 1001

**虽然 HashCode 的倒数第二第三位从  0 变成了 1, 但是运算的结果都是 1001.也就是说, 当 HashMap 长度为 10 的时候, 有些index 结果的出现几率会更大, 而有些 index 结果永远不会出现(比如 0111), 因为二三位是 0, 永远 & 不出 1 来**

**这样, 显然不符合Hash算法均匀分布的原则**

**反观长度 16 或者其他 2 的幂, Length - 1 的值是所有二进制位全为 1, 这种情况下, index 的结果等同于 HashCode 后几位的值.只要输入的 HashCode 本身分布均匀, Hash 算法的结果就是均匀的**

### 高并发下的 HashMap

HashMap的容量是有限的.当经过多次元素插入, 使得HashMap达到一定饱和度时, Key映射位置发生冲突的几率会逐渐提高.

这时候, HashMap需要扩展它的长度, 也就是进行 **Resize**

**影响发生 Resize 的因素有两个: **

1. Capacity

    HashMap的当前长度, HashMap的长度是2的幂, 默认是 16

2. LoadFactor

    HashMap 负载因子, 默认值为 0.75f.

**衡量 HashMap 是否进行 Resize 的条件如下: **

**HashMap.Size  >= Capacity \* LoadFactor**

**HashMap 的 resize 操作要经过下面两个步骤**

1. 扩容

    创建一个新的 Entry 空数组, 长度是原来数组的 2 倍

2. ReHash

    遍历原Entry数组, 把所有的 Entry 重新 Hash 到新数组.为什么要重新 Hash 呢?因为长度扩大以后, Hash 的规则也随之改变.

让我们回顾一下Hash公式: 

**index = HashCode(key) & (Length - 1)**

当原数组长度为 8 时, Hash 运算是和 111B 做与运算; 新数组长度为 16, Hash 运算是和 1111B 做与运算.Hash 结果显然不同.

**Resize 前的 HashMap:**

![http://www.milky.show/images/java/hashmap/hashmap_6.png](http://www.milky.show/images/java/hashmap/hashmap_6.png)

**Resize 后的 HashMap:**

![http://www.milky.show/images/java/hashmap/hashmap_7.png](http://www.milky.show/images/java/hashmap/hashmap_7.png)



**ReHash的Java代码如下**

```java
void transfer(Entry[] newTable, boolean rehash) {
    int newCapacity = newTable.length;
    for (Entry<K,V> e : table) {
        while(null != e) {
            Entry<K,V> next = e.next;
            if (rehash) {
                e.hash = null == e.key ? 0 : hash(e.key);
            }
            int i = indexFor(e.hash, newCapacity);
            e.next = newTable[i];
            newTable[i] = e;
            e = next;
        }
    }
}
```

上述流程在单线程下没有什么问题, 但是 HashMap 并不是线程安全的, 在多线程环境下, rehash 操作会带来线程安全问题

假设一个 HashMap 已经到了 Resize 的临界点.此时有两个线程 A 和 B, 在同一时刻对 HashMap 进行 Put 操作: 

![http://www.milky.show/images/java/hashmap/hashmap_8.png](http://www.milky.show/images/java/hashmap/hashmap_8.png)

![http://www.milky.show/images/java/hashmap/hashmap_9.png](http://www.milky.show/images/java/hashmap/hashmap_9.png)

此时达到 Resize 条件, 两个线程各自进行 Rezie 的第一步, 也就是扩容: 

![http://www.milky.show/images/java/hashmap/hashmap_10.png](http://www.milky.show/images/java/hashmap/hashmap_10.png)

这时候, 两个线程都走到了 ReHash 的步骤.让我们回顾一下 ReHash 的代码: 

![http://www.milky.show/images/java/hashmap/hashmap_11.png](http://www.milky.show/images/java/hashmap/hashmap_11.png)

假如此时线程 B 遍历到 Entry3 对象, 刚执行完红框里的这行代码, 线程就被挂起.对于线程 B 来说: 

**e = Entry3**

**next = Entry2**

这时候线程 A 畅通无阻地进行着 ReHash, 当 ReHash 完成后, 结果如下(图中的 e 和 next, 代表线程 B 的两个引用)

![http://www.milky.show/images/java/hashmap/hashmap_12.png](http://www.milky.show/images/java/hashmap/hashmap_12.png)

直到这一步, 看起来没什么毛病.接下来线程 B 恢复, 继续执行属于它自己的 ReHash.线程 B 刚才的状态是: 

**e = Entry3**

**next = Entry2**

![http://www.milky.show/images/java/hashmap/hashmap_13.png](http://www.milky.show/images/java/hashmap/hashmap_13.png)

当执行到上面这一行时, 显然 i = 3, 因为刚才线程 A 对于 Entry3 的 hash 结果也是 3.

![http://www.milky.show/images/java/hashmap/hashmap_14.png](http://www.milky.show/images/java/hashmap/hashmap_14.png)

我们继续执行到这两行, Entry3 放入了线程 B 的数组下标为 3 的位置, 并且 **e 指向了 Entry2**.此时 e 和 next 的指向如下: 

**e = Entry2**

**next = Entry2**

整体情况如图所示: 

![http://www.milky.show/images/java/hashmap/hashmap_15.png](http://www.milky.show/images/java/hashmap/hashmap_15.png)

接着是新一轮循环, 又执行到红框内的代码行: 

![http://www.milky.show/images/java/hashmap/hashmap_16.png](http://www.milky.show/images/java/hashmap/hashmap_16.png)

**e = Entry2**

**next = Entry3**

整体情况如图所示: 

![http://www.milky.show/images/java/hashmap/hashmap_17.png](http://www.milky.show/images/java/hashmap/hashmap_17.png)

接下来执行下面的三行, 用头插法把 Entry2 插入到了线程B的数组的头结点: 

![http://www.milky.show/images/java/hashmap/hashmap_18.png](http://www.milky.show/images/java/hashmap/hashmap_18.png)

整体情况如图所示: 

![http://www.milky.show/images/java/hashmap/hashmap_19.png](http://www.milky.show/images/java/hashmap/hashmap_19.png)

第三次循环开始, 又执行到红框的代码: 

![http://www.milky.show/images/java/hashmap/hashmap_20.png](http://www.milky.show/images/java/hashmap/hashmap_20.png)

**e = Entry3**

**next = Entry3.next = null**

最后一步, 当我们执行下面这一行的时候, 见证奇迹的时刻来临了: 

![http://www.milky.show/images/java/hashmap/hashmap_21.png](http://www.milky.show/images/java/hashmap/hashmap_21.png)

**newTable[i] = Entry2**

**e = Entry3**

**Entry2.next = Entry3**

**Entry3.next = Entry2**

**链表出现了环形！**

整体情况如图所示: 

![http://www.milky.show/images/java/hashmap/hashmap_22.png](http://www.milky.show/images/java/hashmap/hashmap_22.png)

**此时, 问题还没有直接产生.当调用 Get 查找一个不存在的 Key, 而这个 Key 的 Hash 结果恰好等于 3 的时候, 由于位置 3 带有环形链表, 所以程序将会进入死循环！**



### 容量与哈希

在Java中, 保存数据有两种比较简单的数据结构: 数组和链表.数组的特点是: 寻址容易, 插入和删除困难; 而链表的特点是: 寻址困难, 插入和删除容易.HashMap就是将数组和链表组合在一起, 发挥了两者的优势, 我们可以将其理解为链表的数组.

在HashMap中, 有两个比较容易混淆的关键字段: size和capacity , 这其中capacity就是Map的容量, 而size我们称之为Map中的元素个数.

简单打个比方你就更容易理解了: HashMap就是一个“桶”, 那么容量(capacity)就是这个桶当前最多可以装多少元素, 而元素个数(size)表示这个桶已经装了多少元素.

当我们创建一个HashMap的时候, 如果没有指定其容量, 那么会得到一个默认容量为16的Map, 那么, 这个容量是怎么来的呢?又为什么是这个数字呢?

我们知道, 容量就是一个HashMap中"桶"的个数, 那么, 当我们想要往一个HashMap中put一个元素的时候, 需要通过一定的算法计算出应该把他放到哪个桶中, 这个过程就叫做哈希(hash), 对应的就是HashMap中的hash方法.

我们知道, hash方法的功能是根据Key来定位这个K-V在链表数组中的位置的.也就是hash方法的输入应该是个Object类型的Key, 输出应该是个int类型的数组下标.如果让你设计这个方法, 你会怎么做?

其实简单, 我们只要调用Object对象的hashCode()方法, 该方法会返回一个整数, 然后用这个数对HashMap的容量进行取模就行了.

如果真的是这么简单的话, 那HashMap的容量设置就会简单很多了, 但是考虑到效率等问题, HashMap的hash方法实现还是有一定的复杂的.



### 指定容量初始化

当我们通过 HashMap(int initialCapacity) 设置初始容量的时候, HashMap 并不一定会直接采用我们传入的数值, 而是经过计算, 得到一个新值, 目的是提高 hash 的效率.(1->1、3->4、7->8、9->16)

```java
int n = cap - 1;
n |= n >>> 1;
n |= n >>> 2;
n |= n >>> 4;
n |= n >>> 8;
n |= n >>> 16;
return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
```

上面的算法目的挺简单, 就是: 根据用户传入的容量值(代码中的cap), 通过计算, 得到第一个比他大的 2 的幂并返回.

**总之, HashMap 根据用户传入的初始化容量, 利用无符号右移和按位或运算等方式计算出第一个大于该数的2的幂.**

### 扩容

HashMap 有扩容机制, 就是当达到扩容条件时会进行扩容. HashMap 的扩容条件就是当 HashMap 中的元素个数(size)超过临界值(threshold)时就会自动扩容.

**在 HashMap 中, threshold = loadFactor * capacity.**

**loadFactor 是装载因子, 表示 HashMap 满的程度, 默认值为 0.75f, 设置成 0.75 有一个好处, 那就是 0.75 正好是 3/4, 而capacity 又是 2 的幂.所以, 两个数的乘积都是整数.**

对于一个默认的 HashMap 来说, 默认情况下, 当其 size 大于12(16*0.75)时就会触发扩容, 每次容量扩为原来的 2 倍, 即从 16 扩容到 32, 64, 128...

所以, 通过保证初始化容量均为 2 的幂, 并且扩容时也是扩容到之前容量的 2 倍, 所以, 保证了 HashMap 的容量永远都是2的幂, **这样进行 hash 运算时可以保证很高的效率.**

**在合适的时候扩大数组容量, 再通过一个合适的 hash 算法计算元素分配到哪个数组中, 就可以大大的减少哈希冲突的概率.就能避免查询效率低下的问题, 扩容就是为了减少哈希冲突, 提高性能.**

### 负载因子(loadFactor)为什么是 0.75?

从代码中我们可以看到, 在向 HashMap 中添加元素过程中, 如果 `元素个数(size)超过临界值(threshold)` 的时候, 就会进行自动扩容(resize), 并且, 在扩容之后, 还需要对HashMap中原有元素进行rehash, 即将原来桶中的元素重新分配到新的桶中.

在 HashMap 中, 临界值(threshold) = 负载因子(loadFactor) * 容量(capacity).

**那么, 为什么选择 0.75 呢?背后有什么考虑?为什么不是 1, 不是 0.8?不是 0.5, 而是 0.75 呢?**

**Java 官方的解释说, 默认的负载因子(0.75)在时间和空间成本之间提供了很好的权衡.更高的值减少了空间开销, 但增加了查找成本(反映在 HashMap 类的大多数操作中, 包括 get 和 put).**

试想一下, 如果我们把负载因子设置成 1, 容量使用默认初始值 16, 那么表示一个 HashMap 需要在"满了"之后才会进行扩容.

那么在 HashMap 中, 最好的情况是这 16 个元素通过 hash 算法之后分别落到了 16 个不同的桶中, 否则就必然发生哈希碰撞.而且随着元素越多, 哈希碰撞的概率越大, 查找速度也会越低.

**负载因子表示一个数组可以达到的最大的满的程度.这个值不宜太大, 也不宜太小.**

**loadFactor太大, 比如等于 1, 那么就会有很高的哈希冲突的概率, 会大大降低查询速度.**

**loadFactor太小, 比如等于 0.5, 那么频繁扩容没, 就会大大浪费空间.**

**所以, 这个值需要介于 0.5 和 1 之间.根据数学公式推算.这个值在 log(2) 的时候比较合理.**

**另外, 为了提升扩容效率, HashMap 的容量(capacity)有一个固定的要求, 那就是一定是 2 的幂.**

**所以, 如果 loadFactor 是 3/4 的话, 那么和 capacity 的乘积结果就可以是一个整数.**





### 哈希表如何解决Hash冲突

![http://www.milky.show/images/java/hashmap/hashmap_23.png](http://www.milky.show/images/java/hashmap/hashmap_23.png)

### 为什么HashMap具备下述特点:键-值(key-value)都允许为空、线程不安全、不保证有序、存储位置随时间变化

![http://www.milky.show/images/java/hashmap/hashmap_24.png](http://www.milky.show/images/java/hashmap/hashmap_24.png)

### 为什么 HashMap 中 String、Integer 这样的包装类适合作为 key 键

![http://www.milky.show/images/java/hashmap/hashmap_25.png](http://www.milky.show/images/java/hashmap/hashmap_25.png)

### HashMap 中的 key 若 Object 类型, 则需实现哪些方法?

![http://www.milky.show/images/java/hashmap/hashmap_26.png](http://www.milky.show/images/java/hashmap/hashmap_26.png)



### HashMap之1.7和1.8的区别

1.  底层数据结构不一样, 1.7是**数组 + 链表**, 1.8 则是**数组 + 链表 + 红黑树结构(当链表长度大于 8, 转为红黑树). **
2.  JDK1.8 中 resize() 方法在表为空时, 创建表; 在表不为空时, 扩容; 而 JDK1.7 中 resize() 方法负责扩容, inflateTable() 负责创建表. 
3.   1.8 中没有区分键为 null 的情况, 而 1.7 版本中对于键为 null 的情况调用 putForNullKey() 方法. 但是两个版本中如果键为 null, 那么调用 hash() 方法得到的都将是 0, **所以键为 null 的元素都始终位于哈希表 table[0] 中. **
4.  当 1.8 中的桶中元素处于链表的情况, 遍历的同时最后如果没有匹配的, 直接将节点添加到链表尾部; 而 1.7 在遍历的同时没有添加数据, 而是另外调用了 addEntry() 方法, 将节点添加到链表头部. 
5.  1.7 中新增节点采用头插法, 1.8 中新增节点采用尾插法. 这也是为什么 1.8 不容易出现环型链表的原因. 
6.  1.7 中是通过更改 hashSeed 值修改节点的hash值从而达到rehash时的链表分散, 而 1.8 中键的 hash 值不会改变, rehash 时根据(hash&oldCap)==0 将链表分散. 
7.   1.8 rehash 时保证原链表的顺序, 而 1.7 中 rehash 时有可能改变链表的顺序(头插法导致). 
8.  **在扩容的时候: 1.7 在插入数据之前扩容, 而 1.8 插入数据成功之后扩容. **



### HashMap 的工作原理

HashMap 底层是 hash 数组和单向链表实现, 数组中的每个元素都是链表, 由 Node 内部类(实现 Map.Entry接口)实现, HashMap 通过 put & get 方法存储和获取. 

存储对象时, 将 K/V 键值传给 put() 方法: 

1.  调用 hash(K) 方法计算 K 的 hash 值, 然后结合数组长度, 计算得数组下标; 

2.  调整数组大小(当容器中的元素个数大于 capacity * loadfactor 时, 容器会进行扩容resize 为 2n); 

3.  如果 K 的 hash 值在 HashMap 中不存在, 则执行插入, 若存在, 则发生碰撞; 

    如果 K 的 hash 值在 HashMap 中存在, 且它们两者 equals 返回 true, 则更新键值对; 

    如果 K 的 hash 值在 HashMap 中存在, 且它们两者 equals 返回 false, 则插入链表的尾部(尾插法)或者红黑树中(树的添加方式). 

(JDK 1.7 之前使用头插法、JDK 1.8 使用尾插法)(注意: 当碰撞导致链表大于 TREEIFY_THRESHOLD = 8 时, 就把链表转换成红黑树)

获取对象时, 将 K 传给 get() 方法: ①、调用 hash(K) 方法(计算 K 的 hash 值)从而获取该键值所在链表的数组下标; ②、顺序遍历链表, equals()方法查找相同 Node 链表中 K 值对应的 V 值. 

hashCode 是定位的, 存储位置; equals是定性的, 比较两者是否相等. 





### HashMap 的 hash 算法的实现原理

JDK 1.8 中, 是通过 hashCode() 的高 16 位异或低 16 位实现的: (h = k.hashCode()) ^ (h >>> 16), 主要是从速度, 功效和质量来考虑的, 减少系统的开销, 也不会造成因为高位没有参与下标的计算, 从而引起的碰撞. 



这个也是数学的范畴, 从我们的角度来讲, 只要知道这是为了更好的均匀散列表的下标就好了, 我们来看看 HashMap 的 hash 算法（JDK 8）.

```java
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

乍看一下就是简单的异或运算和右移运算, 但是为什么要异或呢? 为什么要移位呢? 而且移位16? 

在分析这个问题之前, 我们需要先看看另一个事情,  **HashMap 如何根据 hash 值找到数组中的对象**, 我们看看 get 方法的代码: 

```java
final Node<K,V> getNode(int hash, Object key) {
    Node<K,V>[] tab; Node<K,V> first, e; int n; K k;
    if ((tab = table) != null && (n = tab.length) > 0 &&
            (first = tab[(n - 1) & hash]) != null) {
        if (first.hash == hash && // always check first node
                ((k = first.key) == key || (key != null && key.equals(k))))
            return first;
        if ((e = first.next) != null) {
            if (first instanceof TreeNode)
                return ((HashMap.TreeNode<K,V>)first).getTreeNode(hash, key);
            do {
                if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                    return e;
            } while ((e = e.next) != null);
        }
    }
    return null;
}
```

我们看看代码中注释下方的一行代码: first = tab[(n - 1) & hash]). 

使用数组长度减一 与运算 hash 值. 这行代码就是为什么要让前面的 hash 方法移位并异或. 

我们分析一下: 

首先, 假设有一种情况, 对象 A 的 hashCode 为 1000010001110001000001111000000, 对象 B 的 hashCode 为 0111011100111000101000010100000. 

如果数组长度是16, 也就是 15 与运算这两个数,  你会发现结果都是0. 这样的散列结果太让人失望了. 很明显不是一个好的散列算法. 

但是如果我们将 hashCode 值右移 16 位, 也就是取 int 类型的一半, 刚好将该二进制数对半切开. 并且使用位异或运算（如果两个数对应的位置相反, 则结果为1, 反之为0）, 这样的话, 就能避免我们上面的情况的发生. 

总的来说, 使用位移 16 位和 异或 就是防止这种极端情况. 但是, 该方法在一些极端情况下还是有问题, 比如: 10000000000000000000000000 和 10000000001000000000000000 这两个数, 如果数组长度是16, 那么即使右移16位, 在异或, hash 值还是会重复. 但是为了性能, 对这种极端情况, JDK 的作者选择了性能. 毕竟这是少数情况, 为了这种情况去增加 hash 时间, 性价比不高. 



### HashMap 为什么使用 & 与运算代替模运算

我们再看看刚刚说的那个根据hash计算下标的方法: 

tab[(n - 1) & hash]; 

其中 n 是数组的长度. 其实该算法的结果和模运算的结果是相同的. 但是, 对于现代的处理器来说, 除法和求余数（模运算）是最慢的动作. 

上面情况下和模运算相同

a % b == (b-1) & a ,当b是2的指数时, 等式成立. 

我们说 & 与运算的定义: 与运算 第一个操作数的的第n位于第二个操作数的第n位如果都是1, 那么结果的第n为也为1, 否则为0; 

当 n 为 16 时,  与运算 101010100101001001101 时, 也就是
1111 & 101010100101001001000 结果: 1000 = 8
1111 & 101000101101001001001 结果: 1001 = 9
1111 & 101010101101101001010 结果:  1010 = 10
1111 & 101100100111001101100 结果:  1100 = 12

可以看到, 当 n 为 2 的幂次方的时候, 减一之后就会得到 1111* 的数字, 这个数字正好可以掩码. 并且得到的结果取决于 hash 值. 因为 hash 值是1, 那么最终的结果也是1 , hash 值是0, 最终的结果也是0. 



### HashMap 的容量为什么建议是 2 的幂次方

我们说, hash 算法的目的是为了让hash值均匀的分布在桶中（数组）, 那么, 如何做到呢? 试想一下, 如果不使用 2 的幂次方作为数组的长度会怎么样? 

假设我们的数组长度是10, 还是上面的公式: 
1010 & 101010100101001001000 结果: 1000 = 8
1010 & 101000101101001001001 结果: 1000 = 8
1010 & 101010101101101001010 结果:  1010 = 10
1010 & 101100100111001101100 结果:  1000 = 8

看到结果我们惊呆了, 这种散列结果, 会导致这些不同的key值全部进入到相同的插槽中, 形成链表, 性能急剧下降. 

所以说, 我们一定**要保证 & 中的二进制位全为 1**, 才能最大限度的利用 hash 值, 并更好的散列, 只有全是1 , 才能有更多的散列结果. 如果是 1010, 有的散列结果是永远都不会出现的, 比如 0111, 0101, 1111, 1110…, 只要 & 之前的数有 0,  对应的 1 肯定就不会出现（因为只有都是1才会为1）. 大大限制了散列的范围. 



### 我们自定义 HashMap 容量最好是多少

绝对不行, 如果大家看过源码就会发现, **如果Map中已有数据的容量达到了初始容量的 75%, 那么散列表就会扩容**, 而扩容将会重新将所有的数据**重新散列, 性能损失严重**, 所以, 我们可以必须要大于我们预计数据量的 1**.34 倍**, 如果是2个数据的话, 就需要初始化 2.68 个容量. 当然这是开玩笑的, 2.68 不可以, 3 可不可以呢? 肯定也是不可以的, 我前面说了, 如果不是2的幂次方, 散列结果将会大大下降. 导致出现大量链表. 那么我可以将初始化容量设置为4.  当然了, 如果你预计大概会插入 12 条数据的话, 那么初始容量为16简直是完美, 一点不浪费, 而且也不会扩容. 

如果某个map很大, 注意, 肯定是事先没有定义好初始化长度, 假设, 某个Map存储了10000个数据, 那么他会扩容到 20000, 实际上, 根本不用 20000, 只需要 10000* 1.34= 13400 个, 然后向上找到一个2 的幂次方, 也就是 16384 初始容量足够. 

在日常开发中, 可以使用

```java
Map<String, String> map = Maps.newHashMapWithExpectedSize(10);
```

来创建一个HashMap, 计算的过程guava会帮我们完成, 这是一种用内存换性能的做法, 真正使用的时候, 要考虑到内存的影响. 

**在JDK 8中, putAll方法采用了这种方式, 而put、构造函数等并没有默认使用这个公式. **



### HashMap 的 table 的容量如何确定? loadFactor 是什么? 该容量如何变化? 这种变化会带来什么问题? 

1.  table 数组大小是由 capacity 这个参数确定的, 默认是16, 也可以构造时传入, 最大限制是1<<30
2.  loadFactor 是装载因子, 主要目的是用来确认table 数组是否需要动态扩展, 默认值是0.75, 比如table 数组大小为 16, 装载因子为 0.75 时, threshold 就是12, 当 table 的实际大小超过 12 时, table就需要动态扩容
3.  扩容时, 调用 resize() 方法, 将 table 长度变为原来的两倍(注意是 table 长度, 而不是 threshold)

4.  如果数据很大的情况下, 扩展时将会带来性能的损失, 在性能要求很高的地方, 这种损失很可能很致命. 

### HashMap 中 put 方法的过程? 

答: “调用哈希函数获取Key对应的hash值, 再计算其数组下标; 

如果没有出现哈希冲突, 则直接放入数组; 如果出现哈希冲突, 则以链表的方式放在链表后面; 

如果链表长度超过阀值( TREEIFY THRESHOLD==8), 就把链表转成红黑树, 链表长度低于6, 就把红黑树转回链表;

如果结点的key已经存在, 则替换其value即可; 

如果集合中的键值对大于12, 调用resize方法进行数组扩容. ”

### 数组扩容的过程? 

创建一个新的数组, 其容量为旧数组的两倍, 并重新计算旧数组中结点的存储位置. 结点在新数组中的位置只有两种, 原下标位置或原下标+旧数组的大小. 



### 拉链法导致的链表过深问题为什么不用二叉查找树代替, 而选择红黑树? 为什么不一直使用红黑树? 

之所以选择红黑树是为了解决二叉查找树的缺陷, 二叉查找树在特殊情况下会变成一条线性结构(这就跟原来使用链表结构一样了, 造成很深的问题), 遍历查找会非常慢. 推荐: 面试问红黑树, 我脸都绿了. 

而红黑树在插入新数据后可能需要通过左旋, 右旋、变色这些操作来保持平衡, 引入红黑树就是为了查找数据快, 解决链表查询深度的问题, 我们知道红黑树属于平衡二叉树, 但是为了保持“平衡”是需要付出代价的, 但是该代价所损耗的资源要比遍历线性链表要少, 所以当长度大于8的时候, 会使用红黑树, 如果链表长度很短的话, 根本不需要引入红黑树, 引入反而会慢. 



### jdk8 中对 HashMap 做了哪些改变? 

在java 1.8中, 如果链表的长度超过了8, 那么链表将转换为红黑树. (桶的数量必须大于64, 小于64的时候只会扩容)

发生hash碰撞时, java 1.7 会在链表的头部插入, 而java 1.8会在链表的尾部插入

在java 1.8中, Entry被Node替代(换了一个马甲). 



### HashMap & TreeMap & LinkedHashMap 使用场景?

一般情况下, 使用最多的是 HashMap

HashMap: 在 Map 中插入、删除和定位元素时

TreeMap: 在需要按自然顺序或自定义顺序遍历键的情况下

LinkedHashMap: 在需要输出的顺序和输入的顺序相同的情况下



### HashMap 和 HashTable 有什么区别? 

HashMap 是线程不安全的, HashTable 是线程安全的; 

由于线程安全, 所以 HashTable 的效率比不上 HashMap; 

HashMap 最多只允许一条记录的键为 null, 允许多条记录的值为null, 而 HashTable不允许; 

HashMap 默认初始化数组的大小为 16, HashTable 为 11, 前者扩容时, 扩大两倍, 后者扩大两倍 + 1; 

HashMap 需要重新计算 hash 值, 而 HashTable 直接使用对象的 hashCode



### Java 中的另一个线程安全的与 HashMap 极其类似的类是什么? 同样是线程安全, 它与 HashTable 在线程同步上有什么不同? 

ConcurrentHashMap 类(是 Java并发包 java.util.concurrent 中提供的一个线程安全且高效的 HashMap 实现). 

HashTable 是使用 synchronize 关键字加锁的原理(就是对对象加锁); 

而针对 ConcurrentHashMap, 在 JDK 1.7 中采用 分段锁的方式; JDK 1.8 中直接采用了CAS(无锁算法)+ synchronized. 



### HashMap & ConcurrentHashMap 的区别? 

除了加锁, 原理上无太大区别. 另外, HashMap 的键值对允许有 null, 但是 ConCurrentHashMap 都不允许. 



### 为什么 ConcurrentHashMap 比 HashTable 效率要高

HashTable 使用一把锁(锁住整个链表结构)处理并发问题, 多个线程竞争一把锁, 容易阻塞; 

ConcurrentHashMap

-   JDK 1.7 中使用分段锁(ReentrantLock + Segment + HashEntry), 相当于把一个 HashMap 分成多个段, 每段分配一把锁, 这样支持多线程访问. 锁粒度: 基于 Segment, 包含多个 HashEntry. 
-   JDK 1.8 中使用 CAS + synchronized + Node + 红黑树. 锁粒度: Node(首结点)(实现 Map.Entry). 锁粒度降低了. 

### ConcurrentHashMap 在 JDK 1.8 中, 为什么要使用内置锁 synchronized 来代替重入锁 ReentrantLock? 

1.  粒度降低了

2.  JVM 开发团队没有放弃 synchronized, 而且基于 JVM 的 synchronized 优化空间更大, 更加自然. 

3.  在大量的数据操作下, 对于 JVM 的内存压力, 基于 API 的 ReentrantLock 会开销更多的内存. 





## ConcurrentHashMap

简单来说, HashMap 是一个 Entry 对象的数组.数组中的每一个 Entry 元素, 又是一个链表的头节点.

**Hashmap 不是线程安全的.在高并发环境下做插入操作, 有可能出现环型列表导致下一次查询死循环, 想要避免线程安全问题有很多办法, 比如改用 HashTable 或者 Collections.synchronizedMap 方法, 但是这两者有着共同的问题, 性能非常低下, 无论读操作还是写操作, 它们都会给整个集合加锁, 导致同一时间的其他操作阻塞**

在并发环境下, 如何能够兼顾线程安全和运行效率呢, 这时候 ConcurrentHashMap 就应运而生了

**ConcurrentHashMap 其实很简单, 最关键是要理解一个概念: Segment**

同 HashMap 一样, Segment 包含一个 HashEntry 数组, 数组中的每一个 HashEntry 既是一个键值对, 也是一个链表的头节点.

单一的Segment结构如下: 

![http://www.milky.show/images/java/concurrenthashmap/ch_1.png](http://www.milky.show/images/java/concurrenthashmap/ch_1.png)

**像这样的 Segment 对象, 在 ConcurrentHashMap 集合中有多少个呢?有 2 的 N 次方个, 共同保存在一个名为 segments的数组当中.**

因此整个ConcurrentHashMap的结构如下: 

![http://www.milky.show/images/java/concurrenthashmap/ch_2.png](http://www.milky.show/images/java/concurrenthashmap/ch_2.png)

**可以说, ConcurrentHashMap是一个二级哈希表.在一个总的哈希表下面, 有若干个子哈希表.**

**这样的二级结构, 和数据库的水平拆分有些相似.**

ConcurrentHashMap 的优势就是采用了**锁分段技术**, 每一个 Segment 就好比一个自治区, 读写操作高度自治, Segment 之间互不影响

**Case1: 不同Segment的并发写入**

![http://www.milky.show/images/java/concurrenthashmap/ch_3.png](http://www.milky.show/images/java/concurrenthashmap/ch_3.png)

不同Segment的写入是可以并发执行的.

**Case2: 同一Segment的一写一读**

![http://www.milky.show/images/java/concurrenthashmap/ch_4.png](http://www.milky.show/images/java/concurrenthashmap/ch_4.png)

**Case3: 同一Segment的并发写入**

![http://www.milky.show/images/java/concurrenthashmap/ch_5.png](http://www.milky.show/images/java/concurrenthashmap/ch_5.png)

**Segment 的写入是需要上锁的, 因此对同一 Segment 的并发写入会被阻塞.**

**由此可见, ConcurrentHashMap 当中每个 Segment 各自持有一把锁.在保证线程安全的同时降低了锁的粒度, 让并发操作效率更高.**

### Get 方法

1. 为输入的Key做Hash运算, 得到hash值(Hash 的过程实际上是 2 次保证平均)

2. 通过hash值, 定位到对应的Segment对象

3. 再次通过hash值, 定位到Segment当中数组的具体位置.

### PUT 方法

1. 为输入的Key做Hash运算, 得到hash值

2. 通过hash值, 定位到对应的Segment对象

3. 获取可重入锁

4. 再次通过hash值, 定位到Segment当中数组的具体位置

5. 插入或覆盖HashEntry对象

6. 释放锁

从步骤可以看出, ConcurrentHashMap 在读写时都需要二次定位. 首先定位到 Segment, 之后定位到 Segment 内的具体数组下标

### size 方法如何保证一致性

每个 segment 都各自加锁, 在调用 size 方法时怎么解决一致性问题呢

Size方法的目的是统计 ConcurrentHashMap 的总元素数量,  自然需要把各个 Segment 内部的元素数量汇总起来.

**但是, 如果在统计Segment元素数量的过程中, 已统计过的Segment瞬间插入新的元素, 这时候该怎么办呢?**

![http://www.milky.show/images/java/concurrenthashmap/ch_6.png](http://www.milky.show/images/java/concurrenthashmap/ch_6.png)

![http://www.milky.show/images/java/concurrenthashmap/ch_7.png](http://www.milky.show/images/java/concurrenthashmap/ch_7.png)

![http://www.milky.show/images/java/concurrenthashmap/ch_8.png](http://www.milky.show/images/java/concurrenthashmap/ch_8.png)

**ConcurrentHashMap的Size方法是一个嵌套循环, 大体逻辑如下**

1. 遍历所有的 Segment.

2. 把 Segment 的元素数量累加起来.

3. 把 Segment 的修改次数累加起来.

4. 判断所有 Segment 的总修改次数是否大于上一次的总修改次数.如果大于, 说明统计过程中有修改, 重新统计, 尝试次数+1, 如果不是.说明没有修改, 统计结束.

5. 如果尝试次数超过阈值, 则对每一个 Segment 加锁, 再重新统计.

6. 再次判断所有 Segment 的总修改次数是否大于上一次的总修改次数.由于已经加锁, 次数一定和上次相等.

7. 释放锁, 统计结束.

**为什么这样设计呢?这种思想和乐观锁悲观锁的思想如出一辙.**

**为了尽量不锁住所有 Segment, 首先乐观地假设 Size 过程中不会有修改.当尝试一定次数, 才无奈转为悲观锁, 锁住所有Segment 保证强一致性.**



### ConcurrentHashMap 的并发度是什么

程序运行时能够同时更新 ConccurentHashMap 且不产生锁竞争的最大线程数. 默认为 16, 且可以在构造函数中设置. 

当用户设置并发度时, ConcurrentHashMap 会使用大于等于该值的最小2幂指数作为实际并发度(假如用户设置并发度为17, 实际并发度则为32)



## List

### 常见 List 初始化方式

1. **先创建 List 在赋值**

    标准方式, 先创建集合对象, 然后逐个调用`add`方法初始化. 用起来比较繁琐, 不太方便

    ```java
    List<Integer> list = new ArrayList<>();
    list.add(1);
    list.add(2);
    list.add(3);
    ```

2. **使用`{{}}`双大括号初始化**

    使用匿名内部类完成初始化. 外层的`{}`定义了一个ArrayList的匿名内部类, 内层的`{}`定义了一个实例初始化的非静态构造代码块. **有内存泄露风险**

    ```java
    List<Integer> list = new ArrayList(){
        {
            add(1);
            add(2);
            add(3);
        }
    }
    ```

3. **使用 Arrays.asList**

    使用 Arrays 的静态方法 `asList` 初始化. **返回的 list 集合是不可变的**

    ```java
    List<Integer> list = Arrays.asList(1, 2, 3);
    ```

4. **使用 Stream (JDK8 以上)**

    使用 JDK8 引入的 Stream 的 of 方法生成一个 stream 对象, 调用 `collect` 方法进行收集, 形成一个 List 集合

    ```java
    List<Integer> list = Stream.of(1, 2, 3).collect(Collectors.toList());
    ```

5. **使用 Google Guava(需要引入 Guava 工具包)**

    借助 Google Guava 工具集中的 `Lists` 工具类初始化. 需要引入 Guava 才能使用

    ```java
    List<Integer> list = Lists.newArrayList(1, 2, 3);
    ```

6. **使用 Lists(JDK9 以上)**

    使用 JDK9 引入的 `Lists` 完成初始化

    ```java
    List<Integer> list = Lists.of(1, 2, 3);
    ```

