---
layout: post
title: 高性能秒杀
categories: [project]
description: 
keywords: 
---

* content
{:toc}


## 云端部署

### 数据库部署

备份

```
mysqldump -uroot -p --databases seckill > ~/Downloads/seckill.sql
```

导入

```
mysql -uroot -p < /usr/local/seckill.sql
```

### 打包上传

本地打包

```bash
mvn clean package
```

上传到服务器

```bash
scp imooc-seckill-1.0-SNAPSHOT.jar root@122.51.237.121:/usr/local
```

登录服务器创建目录

```bash
mkdir -p /var/www/seckill
```

移动 jar 包

```bash
cd /var/www/seckill
mv /usr/local/imooc-seckill-1.0-SNAPSHOT.jar seckill.jar
```

授权

```bash
chmod -R 777 *
```

启动程序

```bash
java -jar seckill.jar
```



### 编写 deploy

```bash
cd /var/www/seckill
```

编写外挂配置

```bash
vim application.properties

server.port=8088
```

重新启动程序, 指定外挂配置

```bash
java -jar seckill.jar --spring.config.addition-locaotion=/var/www/seckill/application.properties
```

编写 deploy 脚本

```bash
vim deploy.sh

nohup java -Xms400m -Xmx400m -XX:NewSize=200m -XX:MaxNewSize=200m -jar seckill.jar --spring.config.addition-locaotion=/var/www/seckill/application.properties
```



## jemeter 压测

### 线程组

### http 请求

### 查看结果树

### 聚合报告

![http://www.milky.show/images/project/seckill/seckill_4.png](http://www.milky.show/images/project/seckill/seckill_4.png)

90% Line: 90% 的请求落在这个毫秒内, 反之就是有 10% 的请求大于这个毫秒

95% Line: 95% 的请求落在这个毫秒内

99% Line: 95% 的请求落在这个毫秒内

### 查看服务器状态

使用 top 命令可以查看 cpu 的使用状态

```bash
top -H
```

![http://www.milky.show/images/project/seckill/seckill_5.png](http://www.milky.show/images/project/seckill/seckill_5.png)

**CPU usage: 7.12% user, 6.3% sys:** 分表代表用户态和系统态的 CPU 使用率, 加在一起不超过 100%

**Load Avg: 2.88, 3.10, 3.08:** 1 分钟, 5 分钟, 15 分钟 CPU 的使用负载, 不超过服务器 CPU 个数最好, load 越高说明 CPU 耗的越多

## Tomcat 调优

我们使用 5000 个线程循环 20 次对接口 /item/get?id=8 进行压测, 最终会报错显示 tomcat 拒绝连接, 初步判断是 tomcat 的线程数不够导致的该问题

### 查看线程数

通过使用 `pstree -p pid | wc -l` 查看 java 服务的线程数发现只有 39 个线程(29 个其他线程), 导致客户端请求不能建立新的连接发生报错 mac 使用`ps -M pid | wc -l`

### 查看内嵌 tomcat 配置

spring-boot-autoconfigure 包下的 spring-configuration-metadata.json 文件查看各个节点的配置可以看到内嵌 tomcat 的默认配置

**默认内嵌 tomcat 配置**

server.tomact.accept-count: 等待队列长度, 默认 100

server.tomcat.max-connections: 最大可被连接数, 默认 10000

server.tomcat.max-threads: 最大工作线程数, 默认 200

server.tomcat.min-spare-threads: 最小工作线程数, 默认 10

默认配置下, 连接超过 10000 后出现拒绝连接情况

默认配置下, 触发的请求超过 200+100 后拒绝处理

**理解 maxConnections, maxThreads, acceptCount 关系**

我们可以把tomcat比做一个火锅店，流程是取号、入座、叫服务员，可以做一下三个形象的类比：

1. acceptCount 最大等待数

    可以类比为火锅店的排号处能够容纳排号的最大数量；排号的数量不是无限制的，火锅店的排号到了一定数据量之后，服务往往会说：已经客满。

2. maxConnections 最大连接数

    可以类比为火锅店的大堂的餐桌数量，也就是可以就餐的桌数。如果所有的桌子都已经坐满，则表示餐厅已满，已经达到了服务的数量上线，不能再有顾客进入餐厅了。

3. maxThreads 最大线程数

    可以类比为厨师的个数。每一个厨师，在同一时刻，只能给一张餐桌炒菜，就像极了JVM中的一条线程

**整个就餐的流程, 大致如下**

1. 取号: 如果maxConnections连接数没有满，就不需要取号，因为还有空余的餐桌，直接被大堂服务员领上餐桌，点菜就餐即可。如果 maxConnections 连接数满了，但是取号人数没有达到 acceptCount，则取号成功。如果取号人数已达到acceptCount，则拿号失败，会得到Tomcat的Connection refused connect 的回复信息。
2. 上桌: 如果有餐桌空出来了，表示maxConnections连接数没有满，排队的人，可以进入大堂上桌就餐。
3. 就餐: 就餐需要厨师炒菜。厨师的数量，比顾客的数量，肯定会少一些。一个厨师一定需要给多张餐桌炒菜，如果就餐的人越多，厨师也会忙不过来。这时候就可以增加厨师，一增加到上限maxThreads的值，如果还是不够，只能是拖慢每一张餐桌的上菜速度，这种情况，就是大家常见的“上一道菜吃光了，下一道菜还没有上”尴尬场景。

### 修改配置

```properties
server.port=8088
# 等待队列长度, 默认100
server.tomact.accept-count=1000
# 最大工作线程数, 默认200, 4核8g内存, 线程数经验值800
# 操作系统做线程之间的切换调度是有系统开销的, 所以不是越多越好
server.tomcat.max-threads=800
# 最小工作空闲线程数, 默认10, 适当增大一些, 以便应对突然增长的访问量
server.tomcat.min-spare-threads=100
```

**修改参数后启动程序使用 ps -M pid|wc -l 查看发现线程数增多到 129 个, 再次使用 2000 个线程循环 50 次进行压测, 可以看到线程数增加到 832 个, 虽然最终也会报错, 但是不会像默认配置一样很快就发生报错, 说明提高 tomcat 线程数可以增强系统的访问能力**



### 定制化内嵌 Tomcat 开发

**如果我们 keepalive 一直保持连接, 但是网页一直没有操作, 就会占用线程浪费资源**

keepAliveTimeOut: 多少毫秒后不响应的断开 keepalive

maxKeepAliveRequests: 多少次请求后 keepalive 断开失效

使用 WebServerFactoryCustomizer\<ConfigurableServletWebServerFactory\> 定制化内嵌 tomcat 配置

```java
/**
 * 当 Spring 容器内没有 TomcatEmbeddedServletContainerFactory 这个 bean 时，会吧此 bean 加载进 spring 容器中
 *
 * @author miaoqi
 * @date 2020-01-29
 *
 * @return
 */
@Component
public class WebServerConfiguration implements WebServerFactoryCustomizer<ConfigurableWebServerFactory> {

    @Override
    public void customize(ConfigurableWebServerFactory factory) {
        // 使用对应工厂类提供给我们的接口定制化我们的 tomcat connector
        ((TomcatServletWebServerFactory) factory).addConnectorCustomizers(new TomcatConnectorCustomizer() {
            @Override
            public void customize(Connector connector) {
                Http11NioProtocol protocol = (Http11NioProtocol) connector.getProtocolHandler();
                // 定制化keepalivetimeout,设置30秒内没有请求则服务端自动断开keepalive链接
                protocol.setKeepAliveTimeout(30000);
                // 当客户端发送超过10000个请求则自动断开keepalive链接
                protocol.setMaxKeepAliveRequests(10000);
            }
        });
    }

}
```



### 单 Web 容器上线

**线程数量:** 4 核 cpu 8G 内存单进程调度线程数 800-1000以上后即花费巨大的时间在 cpu 调度上

**等待队列长度:** 队列做缓冲池用, 但也不能无限长, 消耗内存, 出队入队也消耗 cpu



### MySql 数据库 QPS 容量问题

**主键查询:** 千万级别数据 = 1-10 毫秒

**唯一索引查询:** 千万级别数据 = 10-100 毫秒

**非唯一索引查询:** 千万级别数据 = 100-1000 毫秒

**无索引:** 百万条数据 = 1000 毫秒+

### MySql 数据库 TPS 容量问题

更新删除操作: 同查询

插入操作: 1W ~ 10W tps(依赖配置优化)





## 分布式扩展

![http://www.milky.show/images/project/seckill/seckill_6.png](http://www.milky.show/images/project/seckill/seckill_6.png)

我使用 600 个线程, 循环 50 次, 10 秒内启动完成, 理论上每秒应该处理 3000 个请求, 而实际上 tps 只有 2500 左右, **证明单机达到了上限, 多余的请求进行排队处理**

**单机容量问题, 水平扩展**

表象: 单机 cpu 使用率增高, memory 占用增加, 网络带宽使用增加

cpu us: 用户空间的 cpu 使用情况(用户层代码)

cpu sy: 内核空间的 cpu 使用情况(系统调用)

load average: 1,5,15 分钟 load 平均值, 根据核数系数, 0 代表通常, 1(核数) 代表打满, 1+代表等待阻塞

memory: free 空闲内存, used 使用内存

### nginx 反向代理负载均衡

到 [http://openresty.org/en](http://openresty.org/en) 下载 openresty(基于 nginx 做了二次封装许多 lua 脚本, 方便使用)

使用 `sbin/nginx -c /usr/local/etc/openresty/nginx.conf` 命令启动 openresty

location 节点 path: 指定 url 映射 key

location 节点内容: root 指定 location path 后对应的根路径, index 指定默认的访问页

sbin/nginx -c conf/nginx.conf 启动

修改配置后直接 sbin/nginx -s reload 无缝重启

#### nginx 部署前端资源

1. 拷贝前端资源到 openresty 的 html/resources 目录下

2. 修改 nginx.conf 配置文件, 将所有 resources/ 请求都映射到 html/resources 目录下

    ```yaml
    server {
      listen       80;
      server_name  localhost;
    
      #charset koi8-r;
    
      #access_log  logs/host.access.log  main;
    
      #location / {
      	#root   html;
      	#index  index.html index.htm;
      #}
      #
      location /resources/ {
      	alias /usr/local/Cellar/openresty/1.15.8.2/nginx/html/resources/;
      	index index.html index.htm;
      }
    
      #error_page  404              /404.html;
    
      # redirect server error pages to the static page /50x.html
      #
      error_page   500 502 503 504  /50x.html;
      	location = /50x.html {
      	root   html;
      }
    
      # proxy the PHP scripts to Apache listening on 127.0.0.1:80
      #
      #location ~ \.php$ {
      #    proxy_pass   http://127.0.0.1;
      #}
    
      # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000
      #
      #location ~ \.php$ {
      #    root           html;
      #    fastcgi_pass   127.0.0.1:9000;
      #    fastcgi_index  index.php;
      #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;
      #    include        fastcgi_params;
      #}
    
      # deny access to .htaccess files, if Apache's document root
      # concurs with nginx's one
      #
      #location ~ /\.ht {
      #    deny  all;
      #}
    }
    ```

#### nginx 反向代理服务端

1. 设置 upstream server 指明后端服务地址

    ```yaml
    upstream backend_server {
    	server 127.0.0.1:8088 weight=1;
    	server 127.0.0.1:8089 weight=1;
    	# 开启 nginx 与 server 的 keepalive, 设置为 30 秒
    	keepalive 30;
    }
    ```

2. 设置动态请求 location 为 proxy pass 路径

    ```yaml
    location / {
    	# 将除去 /resource/ 的请求都当做动态请求, 进行反向代理, 轮询请求后端服务
      proxy_pass http://backend_server;
      # 经过代理 nginx 就成为了 client 端, 需要将原始 Host 传到服务端
      proxy_set_header Host $http_host:$proxy_port;
      # 传入真正的访问 ip
      proxy_set_header X-Real-IP $remote_addr;
      # nginx 作为代理服务器转发了请求
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      
      # 默认使用 http1.0 与 Connection=closed, 即用完就断开连接, 不支持 keepalive
      # 需要手动指定 http1.1 与 Connection="", 即代表支持 keepalive
      proxy_http_version 1.1;
      proxy_set_header Connection "";
    }
    ```

3. 开启 tomcat access log 验证, 修改 springboot 配置文件, 线上建议开启

    ```
    server.tomcat.accesslog.enabled=true
    server.tomcat.accesslog.directory=/Users/miaoqi/Documents/seckill/seckill8088/tomcat
    server.tomcat.accesslog.pattern=%h %l %u %t "%r" %s %b %D
    ```

    %h: 远端 host 即 ip 地址

    %l: 

    %u: 远端 user

    %t: 处理时长

    %r: http 请求的第一行

    %s: 返回状态码

    %b: 请求 response 的大小

    %D: 处理请求的时长

#### nginx 高性能的原因

1. epoll 多路复用

    java bio 模型, 阻塞进程式

    linux select 模型, 变更触发轮询查找, 有 1024 数量上限

    epoll 模型, 变更触发回调直接读取, 理论上无上限

2. master worker 进程模型

    ![http://www.milky.show/images/project/seckill/seckill_7.png](http://www.milky.show/images/project/seckill/seckill_7.png)

    nginx 启动必定会启动一个 master 进程, 随后会根据 nginx.conf 中 worker_processes 的配置数量 fork 出相应数量的 worker 进程

    master 进程监听 80 端口, 当有一个连接请求过来时, 会让 worker 进程在内存中抢锁的方式来决定哪个 worker 进程接管这个 socket, worker 进程执行 accept 操作建立 socket 并且设置回调, 后续所有的操作都在该 worker 中完成

    `nginx -s reload` 操作不会重启 master 进程且已经连接的 socket 不受影响, master 会接管 worker 的 socket, 并且重新启动一个新的 worker 加载配置文件, 再将 socket 交给 worker 进程, 整个过程是在内存中完成的, 效率非常高

3. 协程机制

    依附于线程的内存模型, 切换开销小

    遇阻塞及归还执行权, 代码同步

    无需加锁



### 使用 redis 实现分布式会话存储

token





## 查询优化多级缓存

### 多级缓存的定义

用快速存取设备, 用内存

将缓存推到离用户最近的地方

脏缓存清理

### 掌握 redis 缓存(一级)

单机版

sentinal 哨兵模式

集群 cluster 模式

### 本地热点缓存(二级)

相比于 redis 减少了网络传输的开销, 同时降低了 redis 的压力

本地缓存有如下特点

1. 热点数据(每秒访问上千, 上万的数据)

2. 脏读非常不敏感(每台应用服务器都保存着一份数据, 很可能存在脏读的情况)

3. 内存可控

本地缓存数据比 redis 中的数据的生命周期更短, 是保存在 JVM 中的, 是很宝贵的空间, 在分布式部署中是很难全部清理掉的, 可以用 mq 做广播通知每台服务进行清理

本地缓存本质上是利用一个 Map 存储对应的 key/value 数据, 但是要考虑并发问题, 性能问题, 缓存过期问题, 自己实现起来非常麻烦, 可以使用 Google 提供的 Guava Cache 解决

1. 可控制的大小和超时时间
2. 可配置的 lru 策略
3. 线程安全

### 掌握热点 nginx lua 缓存

#### nginx proxy cache 缓存, 修改 nginx 配置

1. nginx 反向代理前置
2. 依靠文件系统存索引级文件
3. 依靠内存缓存文件地址

```
# 申明一个缓存节点内容
# level 可以作二级目录
# keys_zone: 缓存的名称, 可以任意取
# inactive: 存取 7 天
# max_size: 文件系统最多存放 10g 内容, 达到后采取 lru 算法
proxy_cache_path /usr/local/etc/openresty/tmp_cache levels=1:2 keys_zone=tmp_cache:100m inactive=7d max_size=10g;
```

这种缓存方式是读取本地文件系统, 并不是读取内存数据, 效率反而变低, nginx 还提供了其他的解决方案

#### nginx lua

**nginx 协程**

1. nginx 的每一个 Worker 进程都是在 epoll 或 kqueue 这种事件模型之上, 封装成协程
2. 每一个请求都有一个协程进行处理
3. 即时 ngx_lua 需要运行 lua, 相对 c 有一定的开销, 但依旧能保证高并发能力

**nginx 协程机制**

1. nginx 每个工作进程创建一个 lua 虚拟机
2. 工作进程内的所有协程共享同一个 vm
3. 每个外部请求都由一个 lua 协程处理, 之间数据隔离
4. lua 代码调用 io 等异步接口时, 协程被挂起, 上下文数据
5. 自动保存, 不阻塞工作进程
6. io 异步操作完成后还原协程上下文, 代码继续执行

**nginx 处理阶段**

1. NGX_HTTP_POST_READ_PHASE = 0, // 读取请求头
2. NGX_HTTP_SERVER_REWRITE_PHASE, // 执行 rewirte -> rewrite_handler
3. NGX_HTTP_FIND_CONFIG_PHASE, // 根据 uri 替换 location
4. NGX_HTTP_REWRITE_PHASE, // 根据替换结果继续执行 rewrite -> rewrite_handler
5. NGX_HTTP_POST_REWRITE_PHASE, // 执行 rewrite 后处理
6. NGX_HTTP_PREACCESS_PHASE, // 认证预处理, 请求限制, 连接下肢 -> limit_conn_handler, limit_req_handler
7. NGX_HTTP_ACCESS_PHASE, // 认证处理 -> auth_basic_handler, access_handler
8. NGX_HTTP_POST_ACCESS_PHASE, // 认证后处理, 认证不通过, 丢包
9. NGX_HTTP_TRY_FILES_PHASE, // 尝试 try 标签
10. NGX_HTTP_CONTENT_PHASE, // 内容处理 -> static_handler
11. NGX_HTTP_LOG_PHASE // 日志处理 -> log_handler

## 静态资源 CDN

所有的 ajax 请求都是依附于静态资源 H5 的, 我们访问页面也是先访问 H5 资源, 如果 H5 没有做优化, 依旧不能提高我们的 TPS

假如我们通过 nginx 反向代理, 缓存优化, 通过直接访问后台接口 tps 可以达到 3000, 但是我们通过访问 H5, H5 在访问后台接口时, H5 的 tps 只能到 2000, 那么我们的瓶颈就是在访问 H5 上了, 我们可以使用 CDN 提高对静态资源的访问速度

![http://www.milky.show/images/project/seckill/seckill_8.png](http://www.milky.show/images/project/seckill/seckill_8.png)

### DNS 用 CNAME 解析到源站

通过云服务商进行配置

我们访问 miaoshaserver.chuangzhijian.com 时, DNS 解析出来这是 CNAME 地址, 就会将这条请求发送到 CNAME 地址上, CNAME 对应的服务器会根据请求的 ip 解析出来一个就近的 CDN 节点返回给用户去访问, CDN 节点会判断有没有用户请求的资源,如果有的话就返回, 没有的话就会到源站(nginx)中查找并在自己服务器上保存一份

前端服务和后端应用服务分别使用两套 nginx, 两套域名, 前端的域名使用 CDN 加速提高静态数据的返回速度, 后端不需要

### 回源缓存设置(应用服务)

**cache control 响应头(response header)**

1. private: 客户端可以缓存, 代理服务器不可以缓存
2. public: 客户端和代理服务器都可以缓存
3. max-age=xxx: 缓存的内容将在 xxx 秒后失效
4. no-cache: 强制向服务端再验证一次, 会将对应的缓存存储在客户端, 但是我们在下次用的时候, 要向服务端验证一次这个缓存到底是能用还是不能用再决定是否用这个缓存
5. no-store: 不缓存请求的任何返回内容, 每次都发起新的请求

![http://www.milky.show/images/project/seckill/seckill_9.png](http://www.milky.show/images/project/seckill/seckill_9.png)

**有效性判断(强制验证, 协商逻辑)**

ETag: 资源唯一标识

If-None-Match: 客户端发送的匹配 ETag 标识符

Last-modified: 资源最后被修改的时间

If-Modified-Since: 客户端发送的匹配资源最后修改时间的标识符

![http://www.milky.show/images/project/seckill/seckill_10.png](http://www.milky.show/images/project/seckill/seckill_10.png)

**浏览器三种刷新方式**

回车刷新或 a 链接: 看 cache-control 对应的 max-age 是否仍然有效, 有效则直接 from cache, 若 cache-control 中为 no-cache, 则进入缓存协商逻辑

F5 刷新或 command + r 刷新: 去掉 cache-control 中的 max-age 或直接设置 max-age 为 0, 然后进入缓存协商逻辑

ctrl + F5(windows) 或 command + shift + r(mac) 刷新: 去掉 cache-control 和协商头, 强制刷新

协商机制, 比较 Last-modified 和 ETag 到服务端, 若服务端判断没变化则 304 不返回数据, 否则 200 返回数据



**对于动态请求, 服务端一般会返回 no-cache 但是却不返回 ETag 和 Last-modified 或者不返回该头(max-age=0), 那么下次请求就不会走协商逻辑**



### CDN 自定义缓存策略

虽然源站可以告诉我们可以缓存的时间, 但是 CDN 可以自定义自己的缓存策略

* 可自定义目录过期时间
* 可自定义后缀名过期时间
* 可自定义对应权重
* 可通过界面或 api 强制 cdn 对应目录刷新(非保成功)

![http://www.milky.show/images/project/seckill/seckill_11.png](http://www.milky.show/images/project/seckill/seckill_11.png)

### 静态资源部署策略

css, js, img 等元素使用带版本号部署, 例如 a.js?v=1.0 不便利, 且维护困难

css, js, img 等元素使用带摘要部署, 例如 a.js?v=45edw 存在先部署 html 还是现部署资源的覆盖问题, 有可能导致线上不可用

css, js, img 等元素使用摘要做文件名部署, 开入 45edw.js, 新老笨笨并存且可回滚, 资源部署完后再部署 html(推荐此种)



对应静态资源保持生命周期内不会变, max-age 可设置的很长, 无视失效更新周期

html 文件设置 no-cache 或较短 max-age, 以便于更新

html 文件仍然设置较长的 max-age, 依靠动态的获取版本号请求发送到后端, 异步下载最新的版本号的 html 后展示渲染在前端



动态请求也可以净化成 json 资源推送到 cdn 上

依靠异步请求获取后端节点对应资源状态做紧急下架处理

可通过跑批紧急推送 cdn 内容以使其下架等操作



### 全页面静态化

在服务端完成 html, css, 甚至 js 的 load 渲染成纯 html 文件后直接以静态资源的方式部署到 cdn 上, 部署 cdn 可以有 sdk 使用, 当有数据发生变化时向一个服务推送消息, 这个服务专门用来生成静态页面并且调用 sdk 想 cdn 服务部署静态文件



## 交易性能优化-缓存库存

### 交易性能瓶颈

我们在创建订单的时候, 至少会访问 6 次数据库, 对性能产生了极大的影响

![http://www.milky.show/images/project/seckill/seckill_12.png](http://www.milky.show/images/project/seckill/seckill_12.png)

```java
@Override
@Transactional(rollbackFor = Exception.class)
public OrderModel createOrder(Integer userId, Integer itemId, Integer promoId, Integer amount) throws BusinessException {
    // 1. 校验下单状态,下单的商品是否存在，用户是否合法，购买数量是否正确
    ItemModel itemModel = this.itemService.getItemById(itemId);
    if (itemModel == null) {
        throw new BusinessException(EmBusinessError.PARAMETER_VALIDATION_ERROR, "商品信息不存在");
    }

    UserModel userModel = this.userService.getUserById(userId);
    if (userModel == null) {
        throw new BusinessException(EmBusinessError.PARAMETER_VALIDATION_ERROR, "用户信息不存在");
    }
    if (amount <= 0 || amount > 99) {
        throw new BusinessException(EmBusinessError.PARAMETER_VALIDATION_ERROR, "数量信息不正确");
    }

    // 校验活动信息
    if (promoId != null) {
        //（1）校验对应活动是否存在这个适用商品
        if (promoId.intValue() != itemModel.getPromoModel().getId()) {
            throw new BusinessException(EmBusinessError.PARAMETER_VALIDATION_ERROR, "活动信息不正确");
            //（2）校验活动是否正在进行中
        } else if (itemModel.getPromoModel().getStatus().intValue() != 2) {
            throw new BusinessException(EmBusinessError.PARAMETER_VALIDATION_ERROR, "活动信息还未开始");
        }
    }

    // 2. 落单减库存(还有一种支付减库存, 存在超卖现象)
    boolean result = this.itemService.decreaseStock(itemId, amount);
    if (!result) {
        throw new BusinessException(EmBusinessError.STOCK_NOT_ENOUGH);
    }

    // 3. 订单入库
    OrderModel orderModel = new OrderModel();
    orderModel.setUserId(userId);
    orderModel.setItemId(itemId);
    orderModel.setAmount(amount);
    if (promoId != null) {
        orderModel.setItemPrice(itemModel.getPromoModel().getPromoItemPrice());
    } else {
        orderModel.setItemPrice(itemModel.getPrice());
    }
    orderModel.setPromoId(promoId);
    orderModel.setOrderPrice(orderModel.getItemPrice().multiply(new BigDecimal(amount)));

    // 生成交易流水号,订单号
    orderModel.setId(this.generateOrderNo());
    OrderDO orderDO = this.convertFromOrderModel(orderModel);
    this.orderDOMapper.insertSelective(orderDO);

    // 加上商品的销量
    this.itemService.increaseSales(itemId, amount);
    // 4. 返回前端
    return orderModel;
}
```



### 交易验证优化

#### **用户风控策略优化**

策略缓存模型化, 将用户的验证信息放到 redis 中一份, 减少后续的查库操作

#### **活动校验策略优化**

引入活动发布流程, 模型缓存化, 紧急下线能力

#### **扣减库存优化**

1. 表锁改行锁

    我们在对库存操作时, 没有加索引, 导致会锁住整张表, 需要将 item_id 列加入索引将表锁改为行锁

    ```sql
    update item_stock
    set stock = stock - #{amount}
    where item_id = #{itemId} and stock >= #{amount}
    ```

    ```sql
    ALTER TABLE item_stock ADD UNIQUE INDEX item_id_index(item_id);
    ```

    针对同一个商品在数据库中的扣减库存操作的串行化是不可避免的, 所以我们要进一步优化将数据库操作改为内存操作

2. 扣减库存缓存化

    虽然在内存中扣减库存也是串行的, 但是在内存中的消耗几乎可以忽略不计, 此处要保证内存和数据库的库存一致性

    1. 活动发布同步库存进缓存(同时上架商品)
    2. 下单交易减缓存库存

3. 异步同步数据库

    在内存中扣减库存后, 因为内存中的数据不具备持久性, 需要同步到数据库, 我们采用异步的方式同步

    1. 异步消息扣减数据库库存表, 通过使用 rocketmq

        部署模型

        ![http://www.milky.show/images/project/seckill/seckill_13.png](http://www.milky.show/images/project/seckill/seckill_13.png)

        ![http://www.milky.show/images/project/seckill/seckill_14.png](http://www.milky.show/images/project/seckill/seckill_14.png)

    2. 分布式事务

        ![http://www.milky.show/images/project/seckill/seckill_15.png](http://www.milky.show/images/project/seckill/seckill_15.png)

        我们的系统中, 在 redis 中扣减了内存, 还没有同步到数据库, 这个时候就是**软状态**, 是可以容忍的, 但要保证最终数据库和内存中的数据一致

    3. 保证库存数据库最终一致性

        1. 下载安装 rocketmq

            ```bash
            unzip rocketmq-all-4.6.0-bin-release.zip
            cd rocketmq-all-4.6.0-bin-release
            ```

        2. 启动 name server

            ```bash
            nohup sh bin/mqnamesrv &
            tail -f ~/logs/rocketmqlogs/namesrv.log
            
            lsof -i :9876 查看是否启动 name server
            ```

        3. 启动 broker

            ```bash
            nohup sh bin/mqbroker -n localhost:9876 &
            tail -f ~/logs/rocketmqlogs/broker.log
            ```

        4. 发送/接收消息

            ```bash
             > export NAMESRV_ADDR=localhost:9876
             > sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer
             SendResult [sendStatus=SEND_OK, msgId= ...
            
             > sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer
             ConsumeMessageThread_%d Receive New Messages: [MessageExt...
            ```

        5. 停止服务

            ```bash
            > sh bin/mqshutdown broker
            The mqbroker(36695) is running...
            Send shutdown request to mqbroker(36695) OK
            
            > sh bin/mqshutdown namesrv
            The mqnamesrv(36664) is running...
            Send shutdown request to mqnamesrv(36664) OK
            ```

        6. 创建 topic

            ```bash
            bin/mqadmin updateTopic -n localhost:9876 -t stock -c DefaultCluster
            ```

        7. 修改下单逻辑

#### 目前存在的问题

异步消息发送失败(已解决), catch 异常进行内存回滚

数据库扣减操作执行失败(因为是异步的, 此时订单已经生成了)

下单失败无法正确回补库存(此时已下单, 但是用户迟迟没有支付, 此时无法自动回滚库存)



## 交易性能优化-事务型消息

### 数据类型

主业务数据: master data

操作型数据: log data

### 方案

引入库存操作流水

引入事务性消息机制

### 问题

redis 不可用时如何处理

扣减流水错误如何处理

### 业务场景决定高可用技术实现

每个公司的业务场景都不同, 根据不同的场景讨论不同的解决方案, 假如我们的渠道多, 可以保证发货, 那么就可以存在超卖现象, 如果我们的货物只有这些, 那么宁可少卖, 也不能超卖

我们的代码设计原则是宁可少卖, 不能超卖, 那么解决方案就是如下

* redis 可以比实际数据少, 这样就是少卖的场景
* 超时释放, 如果创建订单中大量假死状态发生, redis 的库存就会远远少于数据库中的值但是也没有实际的订单生成, 我们要有一个后台的补偿机制, 可以将内存中的库存补偿回去



### 库存售罄处理方案

#### 库存售罄标识

当我们的库存售罄后, 要及时打一个标识符, 可以标识某个商品的库存状态

#### 售罄判断是第一步, 售罄后就不去操作任何后续流程

我们的处理逻辑是 当有用户下单时, 第一步就是创建库存流水初始化, 假如我们的商品只有 100 件, 但是有上千万的用户来抢购, 那么每个用户都会创建一个库存流水, 并且绝大多数是无效的, 对数据库是一种很大的浪费, 我们需要首先根据库存售罄标识判断是否有库存, 如果没有库存了, 那么所有的后续流程都不在走了

#### 售罄后通知各个系统售罄

售罄后要通过 mq 及时通知各个系统更新库存状态, 例如我们的商品模型缓存需要更新, 展示给用户售罄状态

#### 回补上新功能

我们还需要一个回补上新的功能, 假如我们给用户一直展示售罄状态也是不可取的

### 后置操作

销量增加, 这个操作也是根据 itemId 来更新数据库的, 会产生行锁的竞争, 这不是一个关键的数据, 不用考虑复杂的业务, 我们可以直接通过消息, 异步化这个销量操作, 就不会因为数据库行锁的性能影响整个下单的过程了



## 流量削峰技术

在单机或双机的情况下, TPS 达到 4000 是很不错的性能

但秒杀在第一秒的时候涌入的流量会远远超出我们的想象, 但是用户并不是第一秒的时候就需要买到商品, 我们需要将第一秒的流量使用一些平滑的方式过渡掉, 削弱峰值, 过渡到第二秒, 甚至第三秒的时候, 使得系统性能平滑的提升

**秒杀接口会被懂技术的用户使用脚本不停的刷, 会影响正常用户的操作**

**秒杀验证逻辑(活动是否开始, 是否有效, 用户是否登录)和秒杀下单接口强关联, 代码冗余度高, 需要拆分成令牌和下单两个接口**

### 秒杀令牌

秒杀接口需要依靠令牌才能进入

秒杀的令牌由秒杀活动模块负责生成, 将活动与下单逻辑拆分开

秒杀活动模块对秒杀令牌生成全权处理, 逻辑收口

秒杀下单前需要先获得秒杀令牌

**缺陷**

秒杀令牌只要活动一开始就可以无限制生成, 假如有 1 亿用户, 那么就会生成 1 亿个令牌, 是非常影响系统性能的, 可以通过秒杀大闸解决该问题

### 秒杀大闸

依靠秒杀令牌的授权原理定制化发牌逻辑, 做到大闸功能

根据秒杀商品初始库存办法对应数量令牌, 控制大闸流量

用户风控策略(判断用户是否合法)前置到秒杀令牌发放中

库存售罄判断前置到秒杀令牌发放中

**缺陷**

浪涌流量涌入后系统无法应对, 假如秒杀商品有 10 万个, 那么我们的大闸就要有 50 万个, 可以发放 50 万个令牌

我们针对单一商品的解决方案就会占用很多资源, 应对多库存, 多商品等令牌限制能力弱

### 队列泄洪

排队有些时候比并发更高效(例如 redis 单线程模型, innodb mutex key 等), 假如我们在并发操作时, 多线程遇到锁竞争的情况, 会导致 CPU 的频繁切换, 性能反而不如队列的排队处理快

依靠排队去限制并发流量(多线程)

依靠排队和下游拥塞窗口程度调整队列释放流量大小

支付宝银行网管队列举例





## 防刷限流

### 验证码技术

包装秒杀令牌前置, 需要验证码来错峰

数学公式验证码生成器

### 限流方案

流量远比你想的要多, 系统活着永远比挂了要好, 宁愿只让少数人能用, 也不要让所有人不能用

#### 限并发

对某一个接口加计数器功能, 该计数器支持并发的加减操作, 每次访问接口减 1, 当减到 0 时就不允许访问了, 简单粗暴, 假如我们限制该接口的并发是 10, 我们 1 秒内进入了 10 个请求, 每个请求都会执行 3 秒, 那么之后的 3 秒计数器就不会增加了, 导致后面的 3 秒接口无法访问, 不可取.

#### 令牌桶算法

**限制每一秒流量的最大值, 可以应对一些突发的流量**, 但是不能超过限定值,  Guava RateLimiter 可以很好地实现, Redis 也可以实现

假设用户访问接口首先需要从桶中获取一个令牌, 并且我们每秒向桶中放 10 个令牌, 那么就可以控制接口访问在 10TPS

![http://www.milky.show/images/project/seckill/seckill_16.png](http://www.milky.show/images/project/seckill/seckill_16.png)

#### 漏桶算法

**平滑网络流量, 以固定的速率流入对应的操作**

假如我们的桶中有 10 滴水, 每秒向外流出 10 滴, 有 10 个用户访问, 就会向桶中注入 10 滴水, 当桶中的水满时就无法流入了, 也可以达到 10TPS 的效果

![http://www.milky.show/images/project/seckill/seckill_17.png](http://www.milky.show/images/project/seckill/seckill_17.png)

### 限流粒度

#### 接口维度

对单一的接口进行限流, 引入令牌桶算法

#### 总维度

假如我们有 10 个接口, 每个接口都可以承受 5TPS 的流量, 那么总共就是 50TPS, 那么我们的系统往往不能承受这么这么多 TPS 的, 因为每个接口都达到了极限值, 那么系统的流量就会达到极限值, 因此我们需要一个总维度的限流, 一般比接口维度的限流小 20%

### 限流范围

#### 集群限流

依赖 redis 或其他的中间件技术做统一计数器, 往往会产生性能瓶颈

#### 单机限流

保证负载均衡的前提下单机平均限流效果更好, Guava RateLimiter

### 防黄牛技术

排队, 限流. 令牌均只能控制总流量, 但无法控制黄牛流量

#### 传统防刷

限制一个会话(session_id, token)同一秒/分钟接口调用次数: 但是黄牛可以开启多个设备刷接口, 这样的话就无效了

限制一个 ip 同一秒钟/分钟接口调用多少次: 数量不好控制, 容易误伤, 我们的企业中往往多个用户是对应同一个 ip 的, 如果针对 ip 进行限制, 很容易误伤正常用户, 黑客也可以伪造 ip

#### 黄牛为什么难防

模拟器作弊: 模拟硬件设备, 可修改设备信息

设备牧场作弊: 工作室里一批移动设备

人工作弊: 靠用尽吸引兼职人员刷单

#### 设备指纹

采集终端设备各项参数, 穷应用是生成唯一设备指纹

根据对应设备指纹的参数猜测出模拟器等可疑设备概率

#### 凭证系统

根据设备指纹下发凭证

关键业务链路上带上凭证并由业务系统到凭证服务器上验证, 凭证系统返回一个可疑率

凭证服务器根据对应凭证所等价的设备指纹参数并根据实时行为风控系统判定对应凭证的可疑度分数

若分数低于某个数值则油业务系统返回固定错误码, 拉起前端验证码验身, 验身成功后加入凭证服务器对应分数



## 登录态管理

### 分布式会话持久性管理

#### 会话策略

在中小型公司可能没有引入 redis, 会将登录态存在数据库中, 每次登陆状态都通过查询数据库来判断, 后期引入 redis 后, 将登录态同步到 redis 中, 当 redis 发生问题时, 降级查询数据库, 看上去是一种完美的设计, 但实际上并不可取, 是非常错误的做法, 在大流量的情况下, 如果 redis 发生问题直接将流量打到数据库上, 会直接导致数据库崩溃

![http://www.milky.show/images/project/seckill/seckill_18.png](http://www.milky.show/images/project/seckill/seckill_18.png)

正确的做法是保证 redis 的高可用引入集群方案, 永远不将流量打到数据库层

![http://www.milky.show/images/project/seckill/seckill_19.png](http://www.milky.show/images/project/seckill/seckill_19.png)

#### 会话有效期

Tomcat 默认为 30m, 是不与服务端发生交互的呆滞时间, 我们使用 redis 管理会话时, 需要自己实现一个会话续命操作, 每次将 token 的有效期延长至 30m, 注意不是无脑加 30m

#### 安全性管理

url query string, get 请求参数内传递 token, 简单方便, 但是安全性非常低

自定义 header 内, 这种方式虽然 token 不会被暴露在 url 上了, 但是在 header 中还是可以被查到, 掩耳盗铃

用安全传输的 https, 将明文转成密文传送, 原则上非常安全, 但是无法避免用户盗取请求的问题, https 只负责传输过程中加密, 未传输前的信息在浏览器还是可以看到

自定义协议, 使用 APP 无法在 H5 端浏览

### 强登录态与弱登录态

#### 强登录态

需要登录, 类似下单时必须要登录才能后续操作

#### 无需登录

浏览商品时, 无需登录就可以操作

#### 弱登录态

类似千人千面的智能推荐, 利用智能算法给每个人展示不同的推荐商品, 如果你没登录, 我就将系统内的热门商品推荐给你, 如果你登陆了, 我就根据算法按照你的喜好推荐商品, 接口可以处理登录态和非登录态请求, 就是弱登录态, 类似的还有购物车功能

### SSO 单点登录

#### 同域名

www.milky.com, www.milky.com

![http://www.milky.show/images/project/seckill/seckill_20.png](http://www.milky.show/images/project/seckill/seckill_20.png)

cookie 基于安全性考虑是不允许跨域传输 cookie 的, 所以简单的做法是请求同一个域名, 根据 url 后缀的不同, 通过 nginx 反向代理将请求路由的不同的应用服务集群, 实现单点登录

#### 根域名相同子域名不同

www.milky.com, api.milky.com

![http://www.milky.show/images/project/seckill/seckill_21.png](http://www.milky.show/images/project/seckill/seckill_21.png)

假如我们买了一个根域名 a.com, 那么我们可以创建无数多个二级域名, api.a.com, alipay.a.com, wechat.a.com, 当用户在 alipay.a.com 登录时服务端返回的 cookie 会默认设置 domain 为 alipay.a.com, 当用户下次访问 alipay.a.com 时会带上 cookie, 而访问 api.a.com 与 wechat.a.com 时不会带上 cookie. 我们可以手动设置 domain 为 /, 代表设置 domain 为 a.com, 我们只要在任意一个域名下登录后拿到 cookie, 在访问其他的二级域名, H5 认定域名后缀是相同的都是 a.com, 就会带上 cookie, 达到单点登录的效果

#### 域名都不相同

www.baidu.com, www.sina.com

使用 OAuth



## MySql 性能优化

### MySql 应用性能优化

#### 缓存

通过本地缓存(GuavaCache), Redis 缓存数据库中的数据, 减少数据库方位

#### 异步

将操作异步化, 错峰处理

#### 批量操作

将 for each(insert into table values(1)) 这种循环单条插入改为 execute once insert into table values (1), (2), (3) 批处理形式

sql 编译 N 次和 1 次的时间与空间复杂度

网络消耗的时间复杂度, 如果采用循环, 会有许多 insert into 字符的浪费

磁盘寻址的复杂度, 批量操作会顺序执行, 不用每次寻址

#### 查询索引

主键查询: 千万条记录 1-10ms

唯一索引: 千万条记录 10-100ms

非唯一索引: 千万条记录 100-1000ms

无索引: 百万条记录 1000ms+

### MySql 单机性能优化

![http://www.milky.show/images/project/seckill/seckill_22.png](http://www.milky.show/images/project/seckill/seckill_22.png)

#### 服务器配置优化

max_connection=1000: 提高 mysql 服务端的连接数, 假如连接数过小, 会导致客户端无法建立连接的情况

innodb_file_per_table=1: 默认是 database 级别, 整个数据库共有一个 datafile, 这个参数可以将 datafile 改为 table 级别, 每个 table 生成一个 datafile, 提高寻址效率

innodb_buffer_pool_size=6G: 对应 data buffer, 当数据写入时, 先写入 buffer, buffer 足够大, 那么命中缓存的概率就非常高, 当读数据时, 如果命中 buffer 中的数据就直接从 buffer 中获取数据, 官方建议配置占数据库服务器内存的 60%~80%

innodb_log_file_size=256M: 对应 undo/redo 日志文件, 当我们的事物在不断地执行, 这个文件就会不断地变大, 当这个文件变大到 256M 时就会有一个重命名的动作, 从新开启一个日志文件, 在这个间隙内, 没有办法执行 flush 操作, 因为文件已经没了, 没有办法 flush, 那么 write ahead log 操作没法做了, 那么客户端的 sql 语句都没法做了, 一旦日志文件发生了时间的切分点, 所有的操作都会 block 掉, 那么 undo/redo buffer 就产生作用了, 但是该值也不能太大, 因为断电重启之后, 会根据这个文件恢复, 如果太大的话, 恢复就会慢, 经验值 256M

innodb_log_buffer_size=16M: 对应 undo/redo buffer, buffer 太大会影响 flush 操作的速度, 经验会 16M

innodb_flush_log_at_trx_commit=2: 需要放在 [mysqld_safe] 节点下

innodb_data_file_path=ibdata1:1G;ibdata2:1G;ibdata3:1G:autoextend, 当 datafile 达到 1G 我们命名为 ibdata1, 再开一个 ibdata2 达到 1G 后再开一个 ibdata3 达到 1G 后就不再管了, 避免单文件过大

## MySql 分布式性能优化

### MySql 主从

单机数据库很容易就达到性能瓶颈, 但是存储服务和我们的应用服务又不太一样, 应用服务是无状态的, 可以水平无限制扩展, 而存储服务因为要保证数据的一致, 很难做到无缝的水平无限制扩展, 所以最简单的办法是主从

![http://www.milky.show/images/project/seckill/seckill_23.png](http://www.milky.show/images/project/seckill/seckill_23.png)

**开启 bin_log**, **事物提交后**, 会将写操作异步记录到 bin log 中, **设置主从同步账号, 配置主从同步,** mysql 也支持半同步策略, 即至少等到一个 slave 同步成功才成功, 要根据业务场景, 如果是交易场景, 那就必须要开启该功能, 保证数据的完整性, 如果业务可以容忍短时间的主从数据不一致风险, 就关闭该功能, 提高性能

理论上 mysql salve 会有 master 的所有数据, 达到一个备份的效果, 另外就是可以做到读写分离, 但是主从同步一定会存在延迟的可能, 一定要有容错方案, 例如从库查不到就到主库去查

mysql 会先写 undo/redo, 假如 undo/redo 成功了, 再去写 bin log 时, 磁盘崩溃, 那么 bin log 就写不成功了, 但此时事物是已经提交了的, mysql 会以 undo/redo 操作为准, 那么 mysql 自身的存储就存在数据不一致问题, 更不要提主从的数据一致y有多么靠谱了, 所以当有问题时, **不能随意将从切换为主**

### 多主多从

主从只是解决了读写分离的压力, 达到备份的效果, 但是写压力终究没有得到一个很好的解决, 我们可以采用多主多从的方式解决写请求的问题, mysql 的多主是将不同的数据分散到不同的 master 节点上, 并不是每个节点都保存相同的数据

#### 数据分片

hash + mod 分片, 确定一个分片键, 对这个键做 hash, 再对这个 hash 做取模运算, 然后路由到相应的主数据库上, 比如订单号生成 hash, 有 10 台 master, 那么就用 hash % 10, 路由到相应的数据库节点上

![http://www.milky.show/images/project/seckill/seckill_24.png](http://www.milky.show/images/project/seckill/seckill_24.png)

#### 数据分片维度

我们做分库分表就是为了将数据分散到不同的数据库上, 以减少插入和查询的压力, 但是因为拆分导致一次用户请求需要跨多个库的时候, 查询的聚合消耗是很不划算的, 因此我们数据库分片维度的设计原则是尽可能保证一次查询的请求几乎全部都命中在一个数据库上, 这样就和查询单库没有区别

* 固定路由位

    例如选取用户 id 作为路由位, 根据用户 id 做 hash, 然后取模选取相应的数据库, 相关的数据都在一个库中查询

* 时间自增分片

    我们可以将数据按月份存放在不同的数据库, 但是需要相关业务配合, 去掉跨月查询

#### 分片冗余一致性保障

假如我们是商户查询, 要将所有交易过的用户查询出来, 就需要横跨所有的数据库查询, 所以要进行数据冗余, 可以拆分成商户订单和用户订单

用户根据 user id 路由, 当 order commit 之后通过 mq 同步到备份服务, 备份服务通过 business id 路由, 就可以将不同用户的订单信息聚合到同一个 business id 对应的数据库中了, 这里要注意处理同步的异常问题, 可以通过 rocket 的事务型消息解决

还可以通过 bin_log 解决

![http://www.milky.show/images/project/seckill/seckill_25.png](http://www.milky.show/images/project/seckill/seckill_25.png)

#### 无迁移扩展

当我们的 master 节点不够用时, 就需要新增节点了, 那么我们 mod 的结果就会发生变化, 所以要进行数据迁移

* mod 位数据迁移

    根据新的 mod 值迁移数据

* 弹性自增

    我们可以在原有的 mod 基础上加一层筛选, 比如 2020-01-01 之前的订单还是按照 2 台 master 进行 mod 运算, 之后的订单按照 4 台 master 进行 mod 运算, 这样就可以无缝的扩容了

## 一致性原理

### 强一致性

最常见的强一致性就是单机数据库的事物管理, ACID, 要么全部成功, 要么全部失败

### 弱一致性

短时间存在数据的不一致, 比如 A 库有这条数据, 但是因为延迟导致 B 库在 3 秒内没有这条数据

### 最终一致性

结合强一致和弱一致又有了最终一致, 在某一个时间点上是弱一致, 但是依靠一些同步机制, 最终能将数据达到强一致, 这就是最终一致性

### CAP

C: 强一致性

A: 高可用性

P: 分区容错性



### BASE

Basic available: 基本可用, 可以容忍挂掉一部分服务, 但是不能全挂

S: 软状态, 对应就是短时间内可以存在数据不一致

E: 最终一致性