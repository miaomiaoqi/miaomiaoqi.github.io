---
layout: post
title: Redis 高可用
categories: [NoSql]
description: 
keywords: 
---


* content
{:toc}




## Redis 主从复制的原理和优化

如果单机部署 Redis 服务, 如果机器发生了故障, 虽然我们可以启动其他的服务迁移过去, 但是机器已经发生了故障, 数据就全部丢失了

Redis 的主从复制可以配置一台机器为 master 节点, 一台机器为 slave 节点, slave 节点会将 master 节点的数据根据一定策略全部拷贝一份, 达到一个备份的效果, 如果 master 节点的机器出现了故障, 那么 slave 节点就可以发挥作用了, Redis 还支持一主多从, 一份数据可以有多份备份, 还可以实现读写分离

一个 master 可以有多个 slave, 一个 slave 只能有一个 master, 数据流向是单向的, master 到 slave

**优点**

- 支持主从复制, 主机会自动将数据同步到从机, 可以进行读写分离；
- 为了分载Master的读操作压力, Slave服务器可以为客户端提供只读操作的服务, 写服务仍然必须由Master来完成；
- Slave同样可以接受其它Slaves的连接和同步请求, 这样可以有效的分载Master的同步压力；
- Master Server是以非阻塞的方式为Slaves提供服务. 所以在Master-Slave同步期间, 客户端仍然可以提交查询或修改请求；
- Slave Server同样是以非阻塞的方式完成数据同步. 在同步期间, 如果有客户端提交查询请求, Redis则返回同步之前的数据；

**缺点**

- Redis不具备自动容错和恢复功能, 主机从机的宕机都会导致前端部分读写请求失败, 需要等待机器重启或者手动切换前端的IP才能恢复；
- 主机宕机, 宕机前有部分数据未能及时同步到从机, 切换IP后还会引入数据不一致的问题, 降低了系统的可用性；
- 如果多个Slave断线了, 需要重启的时候, 尽量不要在同一时间段进行重启. 因为只要Slave启动, 就会发送sync请求和主机全量同步, 当多个 Slave 重启的时候, 可能会导致 Master IO剧增从而宕机. 
- Redis较难支持在线扩容, 在集群容量达到上限时在线扩容会变得很复杂；

### 主从复制配置

Redis 提供了两种主从复制的方式, 一种是使用 slaveof 命令, 另一种是使用配置文件进行配置, 第一次启动时建议使用配置文件, 配置文件方便集中管理, 后续有特殊修改可以使用命令的方式

#### slaveof 命令

如果我们希望 127.0.0.1:6380 成为 127.0.0.1:6379 的从节点, 就使用客户端连接到从节点的 redis 服务器执行命令, 这个命令是一个异步命令

```bash
slaveof 127.0.0.1 6379
```

如果我们需要取消 6380 的从节点, 可以执行如下命令, 但是 6380 原来同步过来的数据不会清除, 只是 6379 不在向 6380 同步数据, 需要手动清空数据, 如果 6380 节点再次执行 slaveof 命令成为其他主节点的从节点, 就会自动清空数据

```
slaveof no one
```

#### 配置文件

| 命令                           | 说明                                | 建议     |
| ------------------------------ | ----------------------------------- | -------- |
| slaveof ip port                | 将当前节点变为 ip port 节点的从节点 |          |
| slave-read-only yes            | 当前节点是不是只读的                | yes      |
| masterauth \<master-password\> | 配置主节点密码                      | 根据情况 |

info replication: 可以查看当前节点的备份信息, 节点状态等信息

### 全量复制和增量复制

#### 主从策略

主从刚刚连接的时候, 进行全量同步; 全同步结束后, 进行增量同步.当然, 如果有需要, slave 在任何时候都可以发起全量同步.redis 策略是, 无论如何, 首先会尝试进行增量同步, 如不成功, 要求从机进行全量同步.

#### 全量同步

Redis 全量复制一般发生在 Slave 初始化阶段, 这时 Slave 需要将 Master 上的所有数据都复制一份. 具体步骤如下

1. 从服务器连接主服务器, 发送 SYNC(Redis2.8 之后叫做 psync)命令, psync ?(run_id第一次不知道) -1(偏移量, 第一次是-1)
2. 从服务器接收主服务器的信息, runId, offset 保存
3. 主服务器接收到 SYNC 命名后, 主服务器 fork 一个子进程执行 BGSAVE 命令生成 RDB 文件, **并使用缓冲区(repl_back_buffer)记录此后执行的所有写命令**
4. 主服务器 BGSAVE 执行完后, 向所有从服务器发送快照文件, 并在发送期间继续记录被执行的写命令
5. 从服务器收到快照文件后丢弃所有旧数据, 载入收到的快照
6. 主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令
7. 从数据库完成对快照的载入, 开始接收命令请求, 并执行来自主数据库缓冲区的写命令;(**从数据库初始化完成**0

**全量复制的开销**

1. 主节点 bgsave 生成 rdb 文件的时间
2. rdb 文件网络传输时间
3. 从节点清空数据时间
4. 从节点加载 rdb 的时间
5. 可能的 aof 重写时间

#### 增量同步

Redis 增量复制是指 Slave 初始化后开始正常工作时主服务器发生的写操作同步到从服务器的过程. 增量复制的过程主要是主服务器每执行一个写命令就会向从服务器发送相同的写命令, 从服务器接收并执行收到的写命令.

1. 主数据库每执行一个写命令就会向从数据库发送相同的写命令, 从数据库接收并执行收到的写命令（**从数据库初始化完成后的操作**）
2. 出现断开重连后, 2.8之后的版本会将断线期间的命令传给重数据库, 增量复制. 
3. 主从刚刚连接的时候, 进行全量同步；全同步结束后, 进行增量同步. 当然, 如果有需要, slave 在任何时候都可以发起全量同步. Redis 的策略是, 无论如何, 首先会尝试进行增量同步, 如不成功, 要求从机进行全量同步. 

<img src="http://www.milky.show/images/redis/redis_86.png" alt="http://www.milky.show/images/redis/redis_86.png" style="zoom:50%;" />



### 开发运维常见问题

#### 读写分离

复制数据延迟, 这个不可避免

读到过期数据, redis 处理过期数据的三种方式是定时过期, 惰性过期, 定期过期, 在主从全量同步时, 有可能会把未过期的数据同步到从节点, 从节点读到过期数据时, 又不能处理过期数据, 因为从节点是只读的, redis3.2 已解决该问题

从节点故障, 需要手动将从节点的客户端迁移到其他节点, 后期使用 Redis Sentinel 解决

#### 主从配置不一致

maxmemory 不一致导致丢失数据

数据结构优化参数(例如 hash-max-ziplist-entries): 因为优化参数不一致, 导致内存大小不一致

#### 规避全量复制

第一次全量复制, 第一次不可避免, maxmemory 不要设置过大, 在低峰时刻进行

主节点重启(运行 id 变化)

复制挤压缓冲区不足

#### 规避复制风暴

一个 master 节点挂载了许多从节点, 当 master 节点重启后, 每个 slave 节点都需要传输一份数据, 要合理规划服务拓扑, Redis Sentinel 可解决



## Redis Sentinel

第一种主从同步/复制的模式, 当主服务器宕机后, 需要手动把一台从服务器切换为主服务器, 这就需要人工干预, 费事费力, 还会造成一段时间内服务不可用. 这不是一种推荐的方式, 更多时候, 我们优先考虑哨兵模式. 

哨兵模式是一种特殊的模式, 首先Redis提供了哨兵的命令, 哨兵是一个独立的进程, 作为进程, 它会独立运行. 其原理是哨兵通过发送命令, 等待Redis服务器响应, 从而监控运行的多个Redis实例. 

<img src="http://www.milky.show/images/redis/redis_87.png" alt="http://www.milky.show/images/redis/redis_87.png" style="zoom: 33%;" />

**哨兵模式的作用:**

- 通过发送命令, 让Redis服务器返回监控其运行状态, 包括主服务器和从服务器
- 当哨兵监测到master宕机, 会自动将slave切换成master, 然后通过**发布订阅模式**通知其他的从服务器, 修改配置文件, 让它们切换主机

然而一个哨兵进程对Redis服务器进行监控, 也可能会出现问题, 为此, 我们可以使用多个哨兵进行监控. 各个哨兵之间还会进行监控, 这样就形成了多哨兵模式. 

<img src="http://www.milky.show/images/redis/redis_88.png" alt="http://www.milky.show/images/redis/redis_88.png" style="zoom: 33%;" />

**故障切换的过程:**

假设主服务器宕机, 哨兵1先检测到这个结果, 系统并不会马上进行 failover 过程, 仅仅是哨兵1主观的认为主服务器不可用, 这个现象成为**主观下线**. 当后面的哨兵也检测到主服务器不可用, 并且数量达到一定值时, 那么哨兵之间就会进行一次投票, 投票的结果由一个哨兵发起, 进行 failover 操作. 切换成功后, 就会通过发布订阅模式, 让各个哨兵把自己监控的从服务器实现切换主机, 这个过程称为**客观下线**. 这样对于客户端而言, 一切都是透明的. 



**哨兵模式的工作方式:**

- 每个Sentinel（哨兵）进程以每秒钟一次的频率向整个集群中的Master主服务器, Slave从服务器以及其他Sentinel（哨兵）进程发送一个 PING 命令. 
- 如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值,  则这个实例会被 Sentinel（哨兵）进程标记为主观下线（SDOWN）
- 如果一个Master主服务器被标记为主观下线（SDOWN）, 则正在监视这个Master主服务器的所有 Sentinel（哨兵）进程要以每秒一次的频率确认Master主服务器的确进入了主观下线状态
- 当有足够数量的 Sentinel（哨兵）进程（大于等于配置文件指定的值）在指定的时间范围内确认Master主服务器进入了主观下线状态（SDOWN）,  则Master主服务器会被标记为客观下线（ODOWN）
- 在一般情况下,  每个 Sentinel（哨兵）进程会以每 10 秒一次的频率向集群中的所有Master主服务器, Slave从服务器发送 INFO 命令
- 当Master主服务器被 Sentinel（哨兵）进程标记为客观下线（ODOWN）时, Sentinel（哨兵）进程向下线的 Master主服务器的所有 Slave从服务器发送 INFO 命令的频率会从 10 秒一次改为每秒一次
- 若没有足够数量的 Sentinel（哨兵）进程同意 Master主服务器下线,  Master主服务器的客观下线状态就会被移除. 若 Master主服务器重新向 Sentinel（哨兵）进程发送 PING 命令返回有效回复, Master主服务器的主观下线状态就会被移除

**优点:**

- 哨兵模式是基于主从模式的, 所有主从的优点, 哨兵模式都具有
- 主从可以自动切换, 系统更健壮, 可用性更高

**缺点:**

- Redis较难支持在线扩容, 在集群容量达到上限时在线扩容会变得很复杂



### 主从复制高可用

主从复制可以使程序的读写分离, 并且达到一个备份的效果, 但是出现故障后只能手动故障转移, 整个流程非常不方便操作, 另外是写能力和存储能力受限

假如 master 宕机了, 主从的增量同步会断掉, master 的客户端的写操作也会失败, 此时读操作还是正常的, 首先手动在一台 slave 节点执行 `slaveof no one` 命令, 让其成为 master 节点, 对其他 slave 节点执行 `slaveof host port` 命令找到新的 master, 所有的客户端都要改变 redis 地址, 整个过程虽然可以用脚本完成, 但是很复杂

### Redis Sentinel 架构

使用 Redis Sentinel 架构, Sentinel 会帮我们监控主从, 集群的信息, 客户端不在直接连接 Redis 服务, 而是连接 Sentinel 服务, 通过 Sentinel 提供给我们可用的 Redis 节点, Sentinel 是集群的, 只要保障了 Sentinel 的高可用, 就可以保证获取正确的 Redis 信息

<img src="http://www.milky.show/images/redis/redis_65.png" alt="http://www.milky.show/images/redis/redis_65.png" style="zoom: 33%;" />

当主从发生了故障, 多个 sentinel 发现并确认 mster 有问题, 选举出一个 sentinel 作为领导, 这个领导选举出一个 slave 作为 master, 通知其余 slave 成为新的 master 的 slave, 通知客户端主从变化, 等待老的 master 复活成为新 master 的 slave

### 安装配置

<img src="http://www.milky.show/images/redis/redis_66.png" alt="http://www.milky.show/images/redis/redis_66.png" style="zoom: 33%;" />

**Redis 主节点配置 redis-7000.conf**

```yaml
daemonize yes
port 7000
dir "/Users/miaoqi/Documents/redis-5.0.7/data"
logfile "7000.log"
pidfile redis-7000.pid
# save 900 1
# save 300 10
# save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump-7000.rdb
appendonly yes
appendfilename appendonly-7000.aof
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
aof-load-truncated yes
```

**Redis 从节点配置 redis-7001.conf, redis-7002.conf**

```yaml
daemonize yes
port 7001
dir "/Users/miaoqi/Documents/redis-5.0.7/data"
logfile "7001.log"
pidfile redis-7001.pid
# save 900 1
# save 300 10
# save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump-7001.rdb
appendonly yes
appendfilename appendonly-7001.aof
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
aof-load-truncated yes
slaveof 127.0.0.1 7000
```

**redis-sentinel-26379.conf, redis-sentinel-26380.conf, redis-sentinel-26381.conf 主要配置, 使用 redis-sentinel 命令启动**

```yaml
daemonize yes
port 26379
dir "/Users/miaoqi/Documents/redis-5.0.7/data"
logfile "26379.log"
pidfile redis-26379.pid
# save 900 1
# save 300 10
# save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump-26379.rdb
appendonly yes
appendfilename appendonly-26379.aof
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
aof-load-truncated yes

# 2 代表至少有多少个 sentinel 发现 mymaster 有问题就进行故障转移
sentinel monitor mymaster 127.0.0.1 7000 2
# sentinel ping mymaster 多少毫秒不通就认为 mymaster 有问题
sentinel down-after-milliseconds mymaster 30000
# 选取新的 master 节点后,  旧的 slave 对新的 master 进行复制时一次复制几个节点
sentinel parallel-syncs mymaster 1
# 故障转移时间
sentinel failover-timeout mymaster 180000
```

### 客户端连接

客户端通过 masterName 获取 sentinel 集合, 选出一个可用的 sentinel 节点, 通过 sentinel 节点获取一个可用的 master 节点, 再次向 master 节点验证是否是 master 节点, 当 Redis 数据节点发生了变化, sentinel 节点会及时通知 client, 采用发布订阅方式

```java
JedisSentinelPool sentinelPool = new JedisSentinelPool(masterName, sentinelSet, poolConfig, timeout);
Jedis jedis = null;
try {
    jedis = redisSentinelPool.getResource();
    // jedis command
} catch (Exception e) {
    logger.error(e.getMessage(), e);
} finally {
    if (jedis != null) {
        jedis.close();
    }
}
```

### 故障转移原理

#### 三个定时任务

每 10 秒每个 sentinel 对 master 和 slave 执行 info

* 发现 slave 节点
* 确认主从关系

每 2 秒每个 sentinel 通过 master 节点的 cahnnel 交换信息(pub/sub)

* 通过__sentinel__:hello 频道交互
* 交互对节点的"看法"和自身信息

每 1 秒每个 sentinel 对其他 sentinel 和 redis 执行 ping

* 心跳检测, 判断失败

#### 主观下线和客观下线

```
sentinel monitor <masterName> <ip> <port> <quorum>
sentinel down-after-milliseconds <masterName> <timeout>
```

主观下线: 每个 sentinel 节点对 Redis 节点失败的"偏见", 有可能是因为网络不通导致, 所以是 sentinel 的主观

客观下线: 所有 sentinel 节点对 Redis 节点失败"达成共识"(超过 quorum 个统一), 通过 `sentinel is-master-down-by-addr` 命令询问其他 sentinel 节点达成共识

**注意:** 需要注意的是 slave 节点只需要主观下线即可, 而 master 节点需要达成客观下线, 因为 slave 节点不需要故障转移

#### 领导者选举

因为发生故障时只需要一个 sentinel 节点完成故障转移即可, 所以需要选举出一个领导者来做这件事

通过 sentinel is-master-down-by-addr 命令客观下线时都希望成为领导者

1. 每个做主观下线的 sentinel 节点向其他 sentinel 节点发送命令, 要求将它设置为领导者
2. 收到命令的 sentinel 节点如果没有同意通过其他 sentinel 节点发送的命令, 那么将同意该请求, 否则拒绝
3. 如果该 sentinel 节点发现自己的票数已经超过 sentinel 集合半数且超过 quorum, 那么它将成为领导者
4. 如果此过程有多个 sentinel 节点成为了领导者, 那么将等待一段时间重新进行选举

#### 故障转移(sentinel 领导者节点完成)

1. 从 slave 节点中选出一个"合适的"节点作为新的 master 节点
2. 对上面的 slave 节点执行 slaveof no one 命令让其成为 master 节点
3. 向剩余的 slave 节点发送命令, 让它们成为新 master 节点的 slave 节点, 复制规则和 parallel-syncs 参数有关
4. 更新对原理 master 节点配置为 slave, 并保持着对其"关注", 当其恢复后命令它去复制新的 master 节点

#### 选择"合适的" slave 节点

1. 选择 slave-priority(slave 节点优先级)最高的 slave 节点, 如果存在则返回, 不存在则继续
2. 选择复制偏移量最大的 slave 节点(复制的最完整), 㘝存在则返回, 不存在则继续
3. 选择 runId 最小的 slave 节点

### 常见开发运维问题

#### 节点下线

例如过保, CPU, 内存, 硬盘等性能不足, 服务不稳定等情况要对某台机器进行下线操作

**主节点**

`sentinel failover <masterName>`  对 sentinel 节点手动执行该命令, 可以跳过主观和客观下线进行故障转移

**从节点**

临时下线还是永久下线, 例如是否做一些清理工作, 但是要考虑读写分离的情况

**Sentinel 节点**

临时下线还是永久下线, 例如是否做一些清理工作, 但是要考虑读写分离的情况

#### 节点上线

**主节点**

sentinel failover 进行替换

**从节点**

slaveof 即可, sentinel 节点可以感知

**sentinel 节点**

参考其他 sentinel 节点直接启动即可



## Redis Cluster

Redis 的哨兵模式基本已经可以实现高可用, 读写分离 , 但是在这种模式下每台 Redis 服务器都存储相同的数据, 很浪费内存, 所以在redis3.0上加入了 Cluster 集群模式, 实现了 Redis 的分布式存储, 也就是说每台 Redis 节点上存储不同的内容. 

* 所有的 redis 节点之间都是互联的(ping-pong 机制), 内部使用二进制协议优化传输速度和带宽
* 节点的 fail 是通过集群中超过半数的节点检测失效时才生效
* 客户端与 redis 节点直连, 不需要中间 proxy 层, 客户端不需要连接集群所有节点, 连接集群中任何一个可用节点即可
* redis-cluster 把所有的物理节点映射到[0-16383]slot 上, cluster 负责维护nodes<>slot<>value
* 单节点 fail: 所有 master 节点投票, 如果超过半数就认为节点挂掉
* 集群 fail: 集群任意 master 挂掉, 如果没有 slave 集群进入 fail 状态, 也可以理解为集群挂掉, 如果集群半数以上 master 挂掉, 无论是否有 slave, 集群都进入 fail 状态

<img src="http://www.milky.show/images/redis/redis_89.png" alt="http://www.milky.show/images/redis/redis_89.png" style="zoom: 33%;" />



### 呼唤集群

Redis 官方声明单节点可以达到 10 万/每秒的 ops, 但是假如我们的业务需要 100 万/每秒呢? 单台机器的内存 16~256G, 如果业务需要 500G 呢? 这时候就需要引入 Redis 集群解决上述问题了

### 数据分布

| 分布方式 | 特点                                                         | 典型产品                                                |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------- |
| 哈希分布 | 数据分散度高<br />键值分布业务无关<br />无法顺序访问<br />支持批量操作 | 一致性哈希 Memcache<br />RedisCluster<br />其他缓存产品 |
| 顺序分布 | 数据分散度易倾斜<br />键值业务相关<br />可顺序访问<br />支持批量操作 | BigTable<br />HBase                                     |

#### 顺序分布

1~10 到一个分区, 11~20 到一个分区...

#### 哈希分区

##### 节点取余分区

hash(key) % nodes, 这种方法非常简单, 但是当我们添加一个节点后, 数据的存储位置就会发生变化, 可以采用多倍扩容减少数据迁移量, 也可以根据时间点选取不同的取模策略

<img src="http://www.milky.show/images/redis/redis_67.png" alt="http://www.milky.show/images/redis/redis_67.png" style="zoom: 33%;" />

##### 一致性哈希分区

##### 虚拟槽分区

每个槽映射一个数据子集, 一般比节点数大, 服务端管理节点, 槽, 数据, 例如 RedisCluster, 必须将槽点和数据都分配给新增的节点才能被使用, 就不会存在丢数据的问题了

<img src="http://www.milky.show/images/redis/redis_68.png" alt="http://www.milky.show/images/redis/redis_68.png" style="zoom: 33%;" />

### 集群架构

#### 节点

与普通节点相比多了如下配置

```yaml
cluster-enabled yes
cluster-config-file nodes-${host}-${port}.conf
```

#### meet 通信

所有节点共享消息, 每 2 个槽都要进行通信

#### 指派槽

假设有 3 个节点, RedisCluster 的虚拟槽范围是 0~16383, 那么达到一个负载均衡的效果就为 A 分配 0~5460, B 分配 5461~10922, C 分配 10923~16383

对客户端来说, 首先要获取到 key 的 hash, keyhash=hash(key), 然后再根据 keyhash 获取槽, slot = keyhash%16383, RedisCluster 会根据 slot 找到对应的机器执行命令

### 搭建集群

#### 原生命令安装

1. 修改配置

    ```yaml
    port ${port}
    daemonize yes
    dir "/Users/miaoqi/Documents/redis-5.0.7/data"
    logfile "7000.log"
    pidfile redis-7000.pid
    dbfilename dump-7000.rdb
    # 开启redis的集群模式
    cluster-enabled yes
    # 集群内节点之间支持最长响应时间
    cluster-node-timeout 15000
    # 配置集群模式下的配置文件
    cluster-config-file nodes-${host}-${port}.conf
    cluster-require-full-coverage no
    ```

    

2. 启动节点

    `redis-server redis-8001.conf`

    `redis-server redis-8002.conf`

    `redis-server redis-8003.conf`

    `redis-server redis-8004.conf`

    `redis-server redis-8005.conf`

    `redis-server redis-8006.conf`

3. meet 操作

    `redis-cli -h 127.0.0.1 -p 8001 cluster meet 127.0.0.1 8002`

    `redis-cli -h 127.0.0.1 -p 8001 cluster meet 127.0.0.1 8003`

    `redis-cli -h 127.0.0.1 -p 8001 cluster meet 127.0.0.1 8004`

    `redis-cli -h 127.0.0.1 -p 8001 cluster meet 127.0.0.1 8005`

    `redis-cli -h 127.0.0.1 -p 8001 cluster meet 127.0.0.1 8006`

4. 分配槽, 需要一个一个分配, 通过脚本遍历

    ```bash
    cluster addslots slot [slot...]
    
    redis-cli -h 127.0.0.1 -p 8001 cluster addslots {0...5461}
    redis-cli -h 127.0.0.1 -p 8002 cluster addslots {5462...10922}
    redis-cli -h 127.0.0.1 -p 8003 cluster addslots {10923...16383}
    ```

    

    ```shell
    sstart=$1
    end=$2
    port=$3
    for slot in `seq ${start} ${end}`
    do
        echo "slot:${slot}"
        redis-cli -p ${port} cluster addslots ${slot}
    done
    ```

    `sh addslots.sh 0 5461 8001`, `sh addslots.sh 5462 10922 8002`, `sh addslots.sh 10923 16383 8003`

5. 设置主从, 8004 拷贝 8001, 8005 拷贝 8002, 8006 拷贝 8003, 通过 cluster nodes 查看 node-id

    ```bash
    cluster replicate node-id
    
    redis-cli -h 127.0.0.1 -p 8004 cluster replicate ${node-id-8001}
    redis-cli -h 127.0.0.1 -p 8005 cluster replicate ${node-id-8002}
    redis-cli -h 127.0.0.1 -p 8006 cluster replicate ${node-id-8003}
    ```

6. 查看集群信息

    查看集群信息: `redis-cli -p 8001 cluster info`

    查看节点 meet 信息: `redis-cli -p 8001 cluster nodes`

    查看槽信息: `redis-cli -p 8001 cluster slots`

    集群模式连接: `redis-cli -c -p 8001 `

    查看 key 对应的槽是多少: `redis-cli -p 8001 cluster keyslot {key}`

#### 官方工具安装

1. 下载, 编译, 安装 ruby

    `brew install ruby`

2. 安装 rubygem redis

    `gem install redis`

3. 使用 redis-trib.rb 安装集群

    当前共 6 个节点, \-\-replicas 1 代表为每个 master 节点指定 1 个从节点, 那么 redis-trib.rb 就会认为前 3 个是 master 节点, 8004 是 8001 的从节点, 8005 是 8002 的从节点..以此类推

    `./redis-trib.rb create --replicas 1 127.0.0.1:8001 127.0.0.1:8002 127.0.0.1:8003 127.0.0.1:8004 127.0.0.1:8005 127.0.0.1:8006`

    redis5.0 变成了如下命令

    `./redis-cli --cluster create 127.0.0.1:8001 127.0.0.1:8002 127.0.0.1:8003 127.0.0.1:8004 127.0.0.1:8005 127.0.0.1:8006 --cluster-replicas 1`

    

### 集群伸缩

#### 伸缩原理

<img src="http://www.milky.show/images/redis/redis_69.png" alt="http://www.milky.show/images/redis/redis_69.png" style="zoom: 33%;" />

#### 扩容集群

1. 准备新节点, redis-8007.conf, redis-8008.conf

    集群模式

    配置和其他节点统一

    启动后是孤儿节点

2. 加入集群, 为它迁移槽和数据实现扩容, 作为从节点负责故障转移

    cluster meet 127.0.0.1 8007

    cluster meet 127.0.0.1 8008

    cluster nodes 查看这两个节点已经变为集群的一员

    建议使用 redis-trib.rb add-node new_host:new_port existing_host:existing_port \-\-slave \-\-master-id

    `redis-trib.rb add-node 127.0.0.1:8007 127.0.0.1:8001`

3. 迁移槽和数据

    对目标节点发送: cluster setslot {slot} importing {sourceNodeId} 命令, 让目标节点准备导入槽的数据

    对源节点发送: cluster setslot {slot} migrating {targetNodeId} 命令, 让源节点准备迁出槽的数据

    源节点循环执行 cluster getkeysinslot {slot} {count} 命令, 每次获取 count 个属于槽的键

    在源节点上执行 migrate {targetIp} {targetPort} key 0 {timeout} 命令把指定 key 迁移

    重复执行步骤 3~4 直到槽下所有的键数据迁移到目标节点

    向洁群内所有主节点发送 cluster setslot {slot} node {targetNodeId} 命令, 通知槽分配给目标节点

    建议使用 `redis-trib.rb reshard 127.0.0.1:8001` 命令迁移槽和数据

#### 缩容集群

1. 迁移槽

    `redis-trib.rb reshard --from {sourceNodeId} --to {targetNodeId} --slots 1366 127.0.0.1:8007`

    需要执行多次将槽分别迁移到其他 master 节点

2. **忘记操作, forget 命令, 先下从节点, 再下主节点, 避免故障转移**

    `redis-trib.rb del-node 127.0.0.1:8001 {offlineNodeId} `

### 客户端路由

#### moved 重定向

<img src="http://www.milky.show/images/redis/redis_70.png" alt="http://www.milky.show/images/redis/redis_70.png" style="zoom: 33%;" />

当我们向集群发送一条命令时, 如果该 key 的槽不在这个节点上, 那么就会抛出 moved 异常, 客户端需要手动重定向到对应的槽中执行命令

`cluster keyslot hello` 可以查看键所对应的槽是多少, 使用 `redis-cli -c -p 8001` 集群模式连接, 可以帮助我们自动跳转到相应的槽中

#### ask 重定向

<img src="http://www.milky.show/images/redis/redis_71.png" alt="http://www.milky.show/images/redis/redis_71.png" style="zoom: 33%;" />

ask 异常发生在槽的迁移过程中, 如果发现槽在当前节点, 但是数据已经被迁移到其他节点了, 就会抛出 ask 异常

#### smart 客户端实现原理

1. 从集群中选一个可运行节点, 使用 cluster slots 初始化槽和节点映射
2. 将 cluster slots 的结果映射到本地, 为每个节点创建 JedisPool
3. 准备执行命令

<img src="http://www.milky.show/images/redis/redis_72.png" alt="http://www.milky.show/images/redis/redis_72.png" style="zoom: 33%;" />

### JedisCluster客户端

#### 基本使用

内部封装了连接池

```java
Set<HostAndPort> nodeList = new HashSet<HostAndPort>();
nodeList.add(new HostAndPort(HOST1,  PORT1));
nodeList.add(new HostAndPort(HOST1,  PORT2));
nodeList.add(new HostAndPort(HOST1,  PORT3));
nodeList.add(new HostAndPort(HOST1,  PORT4));
nodeList.add(new HostAndPort(HOST1,  PORT5));
nodeList.add(new HostAndPort(HOST1,  PORT6));
JedisCluster redisCluster = new JedisCluster(nodeList,  timeout,  poolConfig);
redisCluster.command...
```

#### 使用技巧

1. 定义成单例的, 内置了所有节点的连接池, 保证资源的唯一性, 也不会资源浪费
2. 无需手动借还连接池, JedisCluster 内部已经封装好了
3. 合理设置 commons-pool

### 整合 SpringBoot

### 故障转移

RedisCluster 不需要 RedisSentinel, 因为集群自身实现了高可用, 不需要外部的辅助就可以完成故障发现与故障恢复

#### 故障发现

通过 ping/pong 消息实现故障发现, **不需要 sentinel**

##### 主观下线

某个节点认为另一个节点不可用, 属于"偏见", 不代表所有节点的认知

节点 1 中发送 ping 消息到节点 2, 如果成功回复 pong 消息, 更新与节点 2 的最后通信时间, 如果没有返回, 则断开与节点 2 的连接, 同时会有定时任务, 定时任务判断与节点 2 最后通信时间超过 node-timeout 的时间后, 就标记为 pfail 状态

##### 客观下线

当半数以上持有槽的主节点都标记某节点主观下线

通知集群内所有节点标记故障节点为客观下线

通知故障节点的从节点触发故障转移流程

#### 故障恢复

##### 资格检查

每个从节点检查与故障主节点的断线时间

超过 cluster-node-timeout * cluster-slave-validity-factor 取消资格

cluster-slave-validity-factor 默认是 10

##### 准备选举时间

偏移量更大的节点的准备时间越短, 因为偏移量越大代表数据可能越全, 越优先让该节点成为主节点

##### 选举投票

收集选票, 选票数大于 N/2 + 1, 可替换主节点

##### 替换主节点

1. 当前从节点取消复制变为主节点(slaveof no one)
2. 执行 clusterDelSlot 撤销故障主节点负责的槽, 并执行 clusterAddSlot 把这些槽分配给自己

3. 向集群广播自己的 pong 消息, 表名已经替换了故障从节点

### 开发运维常见问题

#### 集群完整性

主要是根据 `cluster-require-full-coverage` 配置默认为 yes, 如果开启该配置, 代表集群中 16384 个槽都可用, 节点故障或者正在故障转移, 整个集群就不可用了

大多数业务无法容忍某个槽发生问题导致整个集群无法使用, `cluster-require-full-coverage` 建议设置为 no

#### 带宽消耗

官方建议 1000 个节点

消息发送频率: 节点发现与其他节点最后通信时间超过 cluster-node-timeout/2 时会直接发送 ping 消息

消息数据量: slots 槽数组(2kb 空间)和整个集群 1/10 的状态数据(10 个节点状态数据约 1KB)

节点部署的机器规模: 集群分布的机器越多且每台机器酷啊分的节点数越均匀, 则集群内整体的可用带宽越高

**避免使用大集群, 大业务可以多集群**

#### Pub/Sub 广播

#### 数据倾斜

某一个节点上的数据比其他节点要多

##### 节点和槽分配不均

`redis-trib.rb info ip:port` 查看节点, 槽, 键值分布

`redis-trib.rb rebalance ip:port` 进行均衡(谨慎使用)

##### 不同槽对应键值数量差异较大

CRC16 正常情况下比较均匀

可能存在 hash_tag

cluster countkeysinslot {slot} 获取槽对应键值个数

##### 包含 bigkey

例如大字符串, 几百万的元素的 hash, set 等

从节点使用 redis-cli \-\-bigkeys 查找 bigkey

优化数据结构

##### 内存相关配置不一致

hash-max-ziplist-value, set-max-intset-entries

定期"检查"配置一致性

#### 请求倾斜

热点 key: 重要的 key 或者 bigkey

优化

* 避免 bigkey
* 热键不要用 hash_tag
* 当一致性不高时, 可以用本地缓存 + mq

#### 读写分离

只读连接: 集群模式的从节点不接收任何读写请求

* 在从节点发送读命令, 会重定向到负责槽的主节点
* readonly 命令可以读: 这是一个连接级别的命令

读写分离: 更加复杂

* 同样的问题: 复制延迟, 读取过期数据, 从节点故障
* 修改客户端: cluster slaves {nodeId}

#### 数据迁移

官方迁移工具: redis-trib.rb import

* 只能从单机迁移到集群

* 不支持在线迁移: source 需要停写
* 不支持断点续传
* 单线程迁移: 影响性能

在线迁移

* 唯品会 redis-migrate-tool
* 豌豆荚 redis-port

#### 集群 VS 单机

##### 集群限制

key 批量操作支持有限: 例如 mget, mset 必须在一个 slot

key 事务和 Lua 支持有限: 操作的 key 必须在一个节点

key 是数据分区的最小粒度: 不支持 bigkey 分区

不支持多个数据库: 集群模式下只有一个 db0

复制只支持一层: 不支持树形复制结构

##### 思考-分布式 Redis 不一定好

RedisCluster: 满足容量和性能的扩展性, 很多业务"不需要"

* 大多数时客户端性能会"降低"
* 命令无法跨节点使用mget, keys, scan, flush, sinter等
* Lua 和事务无法跨节点使用
* 客户端维护更复杂: sdk 和应用本身消耗(例如更多的连接池)

很多场景 Redis Sentinel 已经足够好



### 集群命令

* 集群(cluster)  

    CLUSTER INFO 打印集群的信息 
    CLUSTER NODES 列出集群当前已知的所有节点(node), 以及这些节点的相关信息.   

* 节点(node)  

    CLUSTER MEET \<ip> \<port> 将 ip 和 port 所指定的节点添加到集群当中, 让它成为集群的一份子. 
    CLUSTER FORGET <node_id> 从集群中移除 node_id 指定的节点. 
    CLUSTER REPLICATE <node_id> 将当前节点设置为 node_id 指定的节点的从节点. 
    CLUSTER SAVECONFIG 将节点的配置文件保存到硬盘里面.   

* 槽(slot) 
    CLUSTER ADDSLOTS \<slot> [slot ...] 将一个或多个槽(slot)指派(assign)给当前节点. 
    CLUSTER DELSLOTS \<slot> [slot ...] 移除一个或多个槽对当前节点的指派. 
    CLUSTER FLUSHSLOTS 移除指派给当前节点的所有槽, 让当前节点变成一个没有指派任何槽的节点. 
    CLUSTER SETSLOT \<slot> NODE <node_id> 将槽 slot 指派给 node_id 指定的节点, 如果槽已经指派给另一个节点, 那么先让另一个节点删除该槽>, 然后再进行指派. 
    CLUSTER SETSLOT \<slot> MIGRATING <node_id> 将本节点的槽 slot 迁移到 node_id 指定的节点中. 
    CLUSTER SETSLOT \<slot> IMPORTING <node_id> 从 node_id 指定的节点中导入槽 slot 到本节点. 
    CLUSTER SETSLOT \<slot> STABLE 取消对槽 slot 的导入(import)或者迁移(migrate).   

* 键 (key) 
    CLUSTER KEYSLOT \<key> 计算键 key 应该被放置在哪个槽上.
    CLUSTER COUNTKEYSINSLOT \<slot> 返回槽 slot 目前包含的键值对数量.
    CLUSTER GETKEYSINSLOT \<slot> \<count> 返回 count 个 slot 槽中的键. 

**这些命令是集群所独有的.执行上述命令要先登录** 